{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#![ML Logo](https://raw.githubusercontent.com/chicochica10/utad-spark-ml/master/images/utad-spark-ml.1x_Banner_300.png)\n",
    "# **Análisis de texto y resolución de entidades**\n",
    "####La resolución de entidades (entity resolution, ER) es un problema clásico pero dificil en la limpieza e integración de datos. En este lab vamos a ver como utilizar Apache Spark para aplicar técnicas de análisis de texto escalables y de resolución de entidades entre dos datasets de productos comerciales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La resolución de entidades (entity resolution -ER- o \"[Record linkage][wiki]\") es el término que se utiliza en estadística para describir el proceso de unir (join) registros de una fuente de datos con otra que describe la misma entidad. Ésto puede incluir procesos de \"desambiguación/enlazado\", \"detección de duplicados\", \"deduplicación\", \"coincidencia de registros\", \"reconciliación\", \"identificación de objetos\", \"integración de datos\" ...\n",
    "#### La resolución de entidades (ER) se refiere a la tarea de encontrar registros en un dataset que que hagan referencia a la misma entidad através de diferentes fuentes de datos (por ejemplo, ficheros de datos, libros, websites, bases de datos). La ER es necesaria cuando se unen datasets basados en entidades que pueden o no compartir un identificador común como por ejemplo una clave de base de datos, una URI, un número nacional de identificación... \n",
    "[wiki]: https://en.wikipedia.org/wiki/Record_linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código\n",
    "#### Para este lab utilizaremos Python básico y transformaciones y acciones de pySpark y la librería de plotting matplotlib.\n",
    "### Ficheros\n",
    "#### Los ficheros de datos están sacados del proyecto [metric-learning](https://code.google.com/p/metric-learning/) y están disponibles en:\n",
    "`utad-spark/lab3`\n",
    "#### El directorio contiene los siguientes ficheros:\n",
    "* **Google.csv**, El dataset de los productos de Google\n",
    "* **Amazon.csv**, El dataset de Amazon\n",
    "* **Google_small.csv**, Muestra de 200 registros de los datos de Google\n",
    "* **Amazon_small.csv**, Muestra de 200 registros de los datos de Amazon\n",
    "* **Amazon_Google_perfectMapping.csv**, Mapeo perfecto (\"gold standard\") de los datos de Amazon y los de Google\n",
    "* **stopwords.txt**, Lista de palabras comunes en inglés\n",
    "#### Además de los ficheros completos de datos hay ficheros de \"muestras\" para cada dataset - usaremos éstos para la **Parte 1**. Existe también un fichero de mapeo perfecto \"gold standard\" que contiene los verdaderos mapeos entre entidades de los dos datasets. Cada fila en el fichero gold standard tiene un par de registros de IDs (uno de Google, uno de Amazon) que pertenecen a dos registros que describen la misma cosa en el mundo real. Usaremos el fichero gold standard para evaluar nuestros algoritmos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 0: Preliminares**\n",
    "#### Leeremos los ficheros y crearemos RDDs formados con las líneas de éstos.\n",
    "#### Por cada fichero de datos (\"Google.csv\", \"Amazon.csv\", y las muestras) parseareamos los IDs de cada registro. Los IDs son la primera columna del fichero (URLs para Google y cadenas alfanuméricas para Amazon). Omitiremos las cabeceras y cargaremos estos ficheros de datos en RDDs de pares donde el *mapping ID* es la clave y el valor es una cadena con el nombre/título, descripción y fabricante para un registro.\n",
    "#### El formato de línea del fichero de Amazon es:\n",
    "   `\"id\",\"title\",\"description\",\"manufacturer\",\"price\"`\n",
    "#### El formato de línea del fichero de Google es:\n",
    "   `\"id\",\"name\",\"description\",\"manufacturer\",\"price\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "DATAFILE_PATTERN = '^(.+),\"(.+)\",(.*),(.*),(.*)'\n",
    "\n",
    "def removeQuotes(s):\n",
    "    \"\"\" Borra las comillas de un string de entrada\n",
    "    Args:\n",
    "        s (str): string de entrada que puede tener \"\" comillas\n",
    "    Returns:\n",
    "        str: un string sin comillas\n",
    "    \"\"\"\n",
    "    return ''.join(i for i in s if i!='\"')\n",
    "\n",
    "\n",
    "def parseDatafileLine(datafileLine):\n",
    "    \"\"\" Parsea una linea de un fichero de datos utilizando el patrón de expresión regular especificado \n",
    "    Args:\n",
    "        datafileLine (str): string de entrada que es una línea del fichero de datos\n",
    "    Returns:\n",
    "        str: un string parseado utilizando la expresion regular dada y sin caracteres de comillas \n",
    "    \"\"\"\n",
    "    match = re.search(DATAFILE_PATTERN, datafileLine)\n",
    "    if match is None:\n",
    "        print 'línea del fichero de datos inválida: %s' % datafileLine\n",
    "        return (datafileLine, -1)\n",
    "    elif match.group(1) == '\"id\"':\n",
    "        print 'Cabecera del fichero de datos: %s' % datafileLine\n",
    "        return (datafileLine, 0)\n",
    "    else:\n",
    "        product = '%s %s %s' % (match.group(2), match.group(3), match.group(4))\n",
    "        return ((removeQuotes(match.group(1)), product), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from test_helper import Test\n",
    "\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('utad-spark', 'lab3')\n",
    "\n",
    "GOOGLE_PATH = 'Google.csv'\n",
    "GOOGLE_SMALL_PATH = 'Google_small.csv'\n",
    "AMAZON_PATH = 'Amazon.csv'\n",
    "AMAZON_SMALL_PATH = 'Amazon_small.csv'\n",
    "GOLD_STANDARD_PATH = 'Amazon_Google_perfectMapping.csv'\n",
    "STOPWORDS_PATH = 'stopwords.txt'\n",
    "\n",
    "def parseData(filename):\n",
    "    \"\"\" Parsea un fichero de datos\n",
    "    Args:\n",
    "        filename (str): nombre del fichero de datos\n",
    "    Returns:\n",
    "        RDD: un RDD de lineas parseadas\n",
    "    \"\"\"\n",
    "    return (sc\n",
    "            .textFile(filename, 4, 0)\n",
    "            .map(parseDatafileLine)\n",
    "            .cache())\n",
    "\n",
    "def loadData(path):\n",
    "    \"\"\" Carga un fichero de datos\n",
    "    Args:\n",
    "        path (str): nombre del fichero de datos\n",
    "    Returns:\n",
    "        RDD: un RDD de lineas parseadas válidas\n",
    "    \"\"\"\n",
    "    filename = os.path.join(baseDir, inputPath, path)\n",
    "    raw = parseData(filename).cache()\n",
    "    failed = (raw\n",
    "              .filter(lambda s: s[1] == -1)\n",
    "              .map(lambda s: s[0]))\n",
    "    for line in failed.take(10):\n",
    "        print '%s - línea del fichero de datos inválida: %s' % (path, line)\n",
    "    valid = (raw\n",
    "             .filter(lambda s: s[1] == 1)\n",
    "             .map(lambda s: s[0])\n",
    "             .cache())\n",
    "    print '%s - Leidas %d líneas, parseadas con éxito %d líneas, falladas al parsear %d líneas' % (path,\n",
    "                                                                                        raw.count(),\n",
    "                                                                                        valid.count(),\n",
    "                                                                                        failed.count())\n",
    "    assert failed.count() == 0\n",
    "    assert raw.count() == (valid.count() + 1)\n",
    "    return valid\n",
    "\n",
    "googleSmall = loadData(GOOGLE_SMALL_PATH)\n",
    "google = loadData(GOOGLE_PATH)\n",
    "amazonSmall = loadData(AMAZON_SMALL_PATH)\n",
    "amazon = loadData(AMAZON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examinemos las líneas que acabamos de cargar de los subsets de muestra - uno de Google y  el otro de Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for line in googleSmall.take(3):\n",
    "    print 'google: %s: %s\\n' % (line[0], line[1])\n",
    "\n",
    "for line in amazonSmall.take(3):\n",
    "    print 'amazon: %s: %s\\n' % (line[0], line[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 1: ER como similitud del texto (Text Similarity) - Bolsa de Palabras (Bags of Words)**\n",
    "####  Un enfoque simple a la resolución de entidades es tratar todos los registros como strings y calcular su similitud con una función de distancia de strings. En esta parte construiremos algunos componentes para realizar análisis de texto por el método de bag-of-words.\n",
    "#### [Bag-of-words][bag-of-words] es un acercamiento simple pero efecivo para realizar análisis de texto.\n",
    "#### La idea es tratar los strings también llamados **documentos** como una *colección no ordenada* de palabras o **tokens**, esto es como una bolsa de palabras (bag of words). \n",
    "> #### **Nota sobre la terminología**: un \"token\" es el resultado de parsear el documento en elementos que consideramos \"atómicos\". Los tokens pueden ser cosas como palabras, números, acrónimos o palabras claves o strings de un tamaño fijo.\n",
    "> #### Las técnicas de Bag of words se aplican a cualquier clase de token de modo que cuando hablamos de \"bag-of-words\" nos referimos a \"bag-of-tokens\" estrictamente hablando.\n",
    "#### Los tokens son pues la unidad atómica para la comparación de textos. Si queremos comparar dos documentos contamos cuantos tokens tienen en común. Si queremos buscar documentos por keywords (que es lo que hace Google) transformamos las keywords en tokens y buscamos documentos que las contengan. Lo bueno de este enfoque es que hace que la comparación de strings  no sea sensible a pequeñas diferencias que probablemente no afecten al significado como por ejemplo signos de puntuación y el orden de las palabras.\n",
    "[bag-of-words]: https://en.wikipedia.org/wiki/Bag-of-words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1(a) Tokenizar un String**\n",
    "#### Implementa la función `simpleTokenize(string)` que toma un string y devuelve una lista de tokens no vacíos en el string.  `simpleTokenize` debe partir (split) strings utilizado la expresión regular proporcionada. Como queremos hacer que el token-matching no sea sensible a las mayúsculas/minúsculas nos aseguraremos que todos los tokens se conviertan a minúsculas.\n",
    "#### Si necesitas ayuda con las expresiones regulares prueba la página [regex101](https://regex101.com/) donde puedes probar de manera interactiva los resultados de aplicar diferentes expresiones regulares a strings.  *\\W incluye el carácter \"_\"*. Usa [re.split()](https://docs.python.org/2/library/re.html#re.split) para realizar la partición del string y asegurate de borrar los tokens vacíos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# https://docs.python.org/2/library/functions.html#filter\n",
    "# http://www.diveintopython.net/power_of_introspection/filtering_lists.html\n",
    "quickbrownfox = 'A quick brown fox jumps over the lazy dog.'\n",
    "split_regex = r'\\W+'\n",
    "\n",
    "def simpleTokenize(string):\n",
    "    \"\"\" Una implementación simple de tokenización de un string de entrada \n",
    "    Args:\n",
    "        string (str): string de entrada\n",
    "    Returns:\n",
    "        list: una lista de tokens\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "print simpleTokenize(quickbrownfox) # debería devolver ['a', 'quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenizar un string (1a)\n",
    "Test.assertEquals(simpleTokenize(quickbrownfox),\n",
    "                  ['a','quick','brown','fox','jumps','over','the','lazy','dog'],\n",
    "                  'simpleTokenize debería tratar el texto de ejemplo')\n",
    "Test.assertEquals(simpleTokenize(' '), [], 'simpleTokenize debería tratar un string vacío')\n",
    "Test.assertEquals(simpleTokenize('!!!!123A/456_B/789C.123A'), ['123a','456_b','789c','123a'],\n",
    "                  'simpleTokenize debería tratar signos de puntuación y dar el resultado en minúsculas')\n",
    "Test.assertEquals(simpleTokenize('fox fox'), ['fox', 'fox'],\n",
    "                  'simpleTokenize no debería borrar duplicados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1b) Borrar stopwords**\n",
    "#### Las *[Stopwords][stopwords]*  son palabras comunes que no contribuyen mucho al significado de un documento (por ejemplo \"the\", \"a\", \"is\", \"to\", etc.). Las stopwords añaden ruido a las comparaciones de la bag-of-words por lo que nomalmente se las excluye.\n",
    "#### Usando el fichero \"stopwords.txt\",  implementa `tokenize`, que será un tokenizador mejorado que no emite stopwords.\n",
    "[stopwords]: https://en.wikipedia.org/wiki/Stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# http://www.diveintopython.net/power_of_introspection/filtering_lists.html\n",
    "stopfile = os.path.join(baseDir, inputPath, STOPWORDS_PATH)\n",
    "stopwords = set(sc.textFile(stopfile).collect())\n",
    "print 'Estas son stopwords: %s' % stopwords\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" Implementación de la tokenización de un string de entrada que excluye stopwords \n",
    "    Args:\n",
    "        string (str): String de entrada\n",
    "    Returns:\n",
    "        list: Una lista de tokens sin stopwords\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "print tokenize(quickbrownfox) # Debería dar ['quick', 'brown', ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Borrado de stopwords (1b)\n",
    "Test.assertEquals(tokenize(\"Why a the?\"), [], 'tokenize debería borrar todas las stopwords')\n",
    "Test.assertEquals(tokenize(\"Being at the_?\"), ['the_'], 'tokenize debería tratar todas las non-stopwords')\n",
    "Test.assertEquals(tokenize(quickbrownfox), ['quick','brown','fox','jumps','lazy','dog'],\n",
    "                    'tokenize debería tratar el texto de ejemplo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1c) Tokenizar los datasets pequeños**\n",
    "#### Vamos a tokenizar los dos conjuntos de datos *pequeños*. Para cada ID en cada uno de los dataset haremos `tokenize` de sus valores y contaremos el número total de tokens.\n",
    "#### ¿Cuántos tokens hay en total en los dos datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "amazonRecToToken = amazonSmall.<RELLENA>\n",
    "googleRecToToken = googleSmall.<RELLENA>\n",
    "\n",
    "def countTokens(vendorRDD):\n",
    "    \"\"\" Cuenta y devuelve el número de tokens\n",
    "    Args:\n",
    "        vendorRDD (RDD of (recordId, tokenizedValue)): Tupla de dos, record ID, valor tokenizado\n",
    "    Returns:\n",
    "        count: cuenta de todos los tokens\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "totalTokens = countTokens(amazonRecToToken) + countTokens(googleRecToToken)\n",
    "print 'Hay %s tokens en los dos datasets combinados' % totalTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Tokenizing the small datasets (1c)\n",
    "Test.assertEquals(totalTokens, 22520, 'totalTokens incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1d) Registro de Amazon con más tokens**\n",
    "#### ¿Cuál es el registro de Amazon con mayor número de tokens?\n",
    "#### En otras palabras queremos ordenar los registros y obtener aquel con mayor número de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "#https://spark.apache.org/docs/1.1.1/api/python/pyspark.rdd.RDD-class.html takeOrdered\n",
    "def findBiggestRecord(vendorRDD):\n",
    "    \"\"\" Encuentra y devuelve el registro con el mayor número de tokens\n",
    "    Args:\n",
    "        vendorRDD (RDD de (recordId, tokens)): Tuplas de dos, ID de registros y tokens\n",
    "    Returns:\n",
    "        list: Una lista de 1 Tupla de dos de ID de registros y tokens\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "biggestRecordAmazon = findBiggestRecord(amazonRecToToken)\n",
    "print 'El registro de Amazon con ID \"%s\" es el que más tokens tiene (%s)' % (biggestRecordAmazon[0][0],\n",
    "                                                                   len(biggestRecordAmazon[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Amazon record with the most tokens (1d)\n",
    "Test.assertEquals(biggestRecordAmazon[0][0], 'b000o24l3q', 'biggestRecordAmazon incorrecto')\n",
    "Test.assertEquals(len(biggestRecordAmazon[0][1]), 1547, 'longitud incorrecta para biggestRecordAmazon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2: ER como Similitud de Texto - Bang-of-Words ponderada utilizando TF-IDF**\n",
    "#### Las comparaciones utilizando Bag-of-words no son muy buenas cuando todos los tokens tienen el mismo peso: Algunos tokens son más importantes que otros. Los pesos nos darán una manera de especificar a que tokens favorecer. Cuando comparemos documentos, en vez de contar tokens  en común utilizaremos los pesos para favorercer aquellos tokens con mayor valor de ponderación. Una buena asignación heurística de pesos que se suele utilizar es la \"Frecuencia de términos / Frecuencia de documentos inversa\" (Term-Frequency / Inverse-Document-Frecuenciy) o [TF-IDF][tfidf]\n",
    "\n",
    "#### **TF**\n",
    "#### TF valora más los tokens que aparecen más veces en un mismo documento. Se calcula como la frecuencia de un token en un documento, por ejemplo, si el documento *d* contiene 100 tokens y el token *t* aparece en *d* 5 veces, entonces  el peso TF de *t* en *d* es *5/100 = 1/20*. La intuición para TF es que si una palabra aparece frecuentemente en un documento es más importante para el significado del documento. \n",
    "\n",
    "#### **IDF**\n",
    "#### IDF recompensa los tokens que son raros en un dataset. La intuición es que es más significativo que dos documentos compartan una palabra rara que una palabra común. El peso IDF para un token *t* en un conjunto de documentos, *U* se calcula asi:\n",
    "* #### Sea *N* el total de número de documentos en *U*\n",
    "* #### Encontrar *n(t)*, el número de documentos en *U* que contienen *t*\n",
    "* #### Entonces *IDF(t) = N/n(t)*.\n",
    "####  *n(t)/N* es la frecuencia de *t* en *U*, y *N/n(t)* es la inversa de la frecuencia.\n",
    "> #### **Nota sobre la terminología**: Algunas veces los pesos de los tokens dependen del documento al que el token pertenece, esto es, el mismo token puede tener diferentes pesos cuando se encuentra en diferentes documentos. Llamamos a estos pesos *locales*. TF es un ejemplo de peso local porque depende de la longitud de la fuente. Por otro lado, el peso de algunos tokenes sólamente dependeran del token y es el mismo donde quiera que encontremos ese token. Llamamos a estos pesos *globales*. IDF es de este tipo.\n",
    "#### **TF-IDF**\n",
    "#### Por último, el peso total TF-IDF de un token en un documento es el producto de sus pesos TF e IDF.\n",
    "[tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2a) Implementar una función TF**\n",
    "#### Implementar `tf(tokens)`  que toma una lista de tokens y devuelve un [dictionary](https://docs.python.org/2/tutorial/datastructures.html#dictionaries)  de Python que mapea tokens a pesos TF.\n",
    "#### Los pasos que esta función debe realizar son:\n",
    "* #### Crear un diccionario vacío de Python\n",
    "* #### Para cada token en la lista de `tokens`, contar 1 por cada ocurrencia y añadir el token al diccionario\n",
    "* #### Por cada token en el diccionario, dividir la cuenta del token por el número total de tokenes en la lista de entrada `tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def tf(tokens):\n",
    "    \"\"\" Calcula TF\n",
    "    Args:\n",
    "        tokens (lista de str): Lista de tokens desde tokenize\n",
    "    Returns:\n",
    "        dictionary: un diccionario de tokens  a sus valores TF\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "    return <RELLENA>\n",
    "\n",
    "print tf(tokenize(quickbrownfox)) # debería dar { 'quick': 0.1666 ... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implementar una función TF (2a)\n",
    "tf_test = tf(tokenize(quickbrownfox))\n",
    "Test.assertEquals(tf_test, {'brown': 0.16666666666666666, 'lazy': 0.16666666666666666,\n",
    "                             'jumps': 0.16666666666666666, 'fox': 0.16666666666666666,\n",
    "                             'dog': 0.16666666666666666, 'quick': 0.16666666666666666},\n",
    "                    'resultado incorrecto para tf en el texto de muestra')\n",
    "tf_test2 = tf(tokenize('one_ one_ two!'))\n",
    "Test.assertEquals(tf_test2, {'one_': 0.6666666666666666, 'two': 0.3333333333333333},\n",
    "                    'resultado incorrecto para el test tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2b) Crear un corpus**\n",
    "#### Crea un RDD de tuplas de dos llamado `corpusRDD` que sea una combinación de los dos datasets pequeños `amazonRecToToken` y `googleRecToToken`. cada elemento del `corpusRDD` debería ser un par compuesto por una clave de uno de los datasets pequeños (ID o URL) y el valor el asociado a esa clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "corpusRDD = <RELLENA>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST crear un corpus (2b)\n",
    "Test.assertEquals(corpusRDD.count(), 400, 'corpusRDD.count() incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2c) Implementar una función IDF**\n",
    "#### Implementa `idfs` que asigna un peso IDF a cada token único en un RDD llamado `corpus`. La función debería devolver un RDD de pares donde la `clave` es el token único y el valor es el peso IDF para ese token. \n",
    "#### El peso IDF para un token *t* en un conjunto de documentos *U* se calcula asi:\n",
    "* #### Sea *N* el total de número de documentos en *U*\n",
    "* #### Encontrar *n(t)*, el número de documentos en *U* que contienen *t*\n",
    "* #### Entonces *IDF(t) = N/n(t)*.\n",
    "#### Los pasos que la función debería realizar son:\n",
    "* #### Calcular *N*. Piensa como puedes calcular *N* desde el RDD de entrada.\n",
    "* #### Crear un RDD (*no un RDD de pares*) que contenga un token único para cada documento en la entrada `corpus`. Para cada documento, deberías incluir el token una única vez *incluso aunque aparezca múltiples veces en ese documento.*\n",
    "* #### Para cada token único, contar cuantas veces aparece en el documento y calcular IDF para ese token: *N/n(t)*\n",
    "#### Utiliza los `idfs` para calcular los pesos IDF para todos los tokens en `corpusRDD` (los dataset combinados pequeños).\n",
    "#### ¿Cuántos tokens únicos hay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def idfs(corpus):\n",
    "    \"\"\" Calcula IDF\n",
    "    Args:\n",
    "        corpus (RDD): corpus de entrada\n",
    "    Returns:\n",
    "        RDD: un RDD de (token, valor IDF )\n",
    "    \"\"\"\n",
    "    N = <RELLENA>\n",
    "    uniqueTokens = corpus.<RELLENA>\n",
    "    tokenCountPairTuple = uniqueTokens.<RELLENA>\n",
    "    tokenSumPairTuple = tokenCountPairTuple.<RELLENA>\n",
    "    return (tokenSumPairTuple.<RELLENA>)\n",
    "\n",
    "idfsSmall = idfs(amazonRecToToken.union(googleRecToToken))\n",
    "uniqueTokenCount = idfsSmall.count()\n",
    "\n",
    "print 'Hay %s tokens únicos en los datasets pequeños.' % uniqueTokenCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implemetar una función IDF (2c)\n",
    "Test.assertEquals(uniqueTokenCount, 4772, 'uniqueTokenCount incorrecto')\n",
    "tokenSmallestIdf = idfsSmall.takeOrdered(1, lambda s: s[1])[0]\n",
    "Test.assertEquals(tokenSmallestIdf[0], 'software', 'El token IDF más pequeño es incorrecto')\n",
    "Test.assertTrue(abs(tokenSmallestIdf[1] - 4.25531914894) < 0.0000000001,\n",
    "                'valor del mas pequeño de IDF incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2d) Tokens con el IDF más pequeño**\n",
    "#### Imprime los 11 tokens con el IDF más pequeño en el dataset combinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smallIDFTokens = idfsSmall.takeOrdered(11, lambda s: s[1])\n",
    "print smallIDFTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2e) Histograma IDF**\n",
    "#### Dibuja un histograma con los valores IDF. Se asegura del escalado y empaquetado de los datos usando la librería `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "small_idf_values = idfsSmall.map(lambda s: s[1]).collect()\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "plt.hist(small_idf_values, 50, log=True)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(2f) Implementar la función TF-IDF**\n",
    "#### Utiliza la función `tf` para implementar una función `tfidf(tokens, idfs)` que tome un lista de tokens de un documento un dicionario de Python de pesos IDF y devuelva un dicionario de Python que mapee los tokens individuales a pesos TF-IDF.\n",
    "#### Los pasos que la funcion debe realizar son:\n",
    "* #### Calcular la frecuencia de los tokens para `tokens`\n",
    "* #### Crear un dicionario Python donde cada token se mapee a su frecuencia de token por su peso IDF.\n",
    "#### Utilizar la función `tfidf` para calcular los pesos del producto de Amazon 'b000hkgj8k'. Para ello necesitamos extraer ese registro del dataset pequeño de Amazon tokenizado y convertir los IDFs del dataset pequeño de Amazon en un diccionario Python. Para la primer parte podemos usar una transformación `filter()` para extraer el registro y una acción `collect()`para devolver el valor al driver. Para la segunda parte usaremos la acción [`collectAsMap()`](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collectAsMap) para devolver los IDFs al driver como un diccionario Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Calcula TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): lista de tokens de tokenize\n",
    "        idfs (dictionary): diccionario con los valores IDF\n",
    "    Returns:\n",
    "        dictionary: un diccionario de registros con valores TF-IDF\n",
    "    \"\"\"\n",
    "    tfs = <RELLENA>\n",
    "    tfIdfDict = <RELLENA>\n",
    "    return tfIdfDict\n",
    "\n",
    "recb000hkgj8k = amazonRecToToken.filter(lambda x: x[0] == 'b000hkgj8k').collect()[0][1]\n",
    "idfsSmallWeights = idfsSmall.collectAsMap()\n",
    "rec_b000hkgj8k_weights = tfidf(recb000hkgj8k, idfsSmallWeights)\n",
    "\n",
    "print 'El registro Amazon \"b000hkgj8k\" tiene los siguientes tokens y pesos:\\n%s' % rec_b000hkgj8k_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implementar la función TF-IDF (2f)\n",
    "Test.assertEquals(rec_b000hkgj8k_weights,\n",
    "                   {'autocad': 33.33333333333333, 'autodesk': 8.333333333333332,\n",
    "                    'courseware': 66.66666666666666, 'psg': 33.33333333333333,\n",
    "                    '2007': 3.5087719298245617, 'customizing': 16.666666666666664,\n",
    "                    'interface': 3.0303030303030303}, 'rec_b000hkgj8k_weights incorrectos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 3: ER como Similutud de Texto - Similitud del Coseno (Cosine Similarity)**\n",
    "#### Ahora estamos listos para hacer comparaciones de texto de una manera formal. Usaremos la métrica **[cosine similarity][cosine]** para medir la distancia entre strings. Trataremos cada documento como un vector en un espacio n-dimensional y compararemos los dos documentos calculando el coseno del ángulo entre los dos vectores. Es *más* fácil de lo que parece.\n",
    "\n",
    "#### La primera cuestión a resolver es como representar los documentos como vectores. la Respuesta es fácil: ¡Bag-of-words! Trataremos cada token único como una dimensión y los pesos como magnitudes en cada una de las dimensiones. Por ejemplo, supongamos que utilizamos la cuenta simple como pesos y queremos interpretar la cadena \"Hello, world! Goodbay, world!\" como un vector. \"Hello\" y \"goodby\" son dimensiones del vector con valor 1 y la dimensión \"world\" tiene valor 2 y el resto de las otras dimensiones serán cero.\n",
    "#### La siguiente cuestion es: Dados dos vectores como calcularemos el coseno del ángulo entre ellos. Para eso utilizarmos la fórmula del producto escalar entre dos vectores (dot product):\n",
    "#### $$ a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta $$\n",
    "#### Donde $ a \\cdot b = \\sum a_i b_i $ es el producto escalar de dos vectores, y$ \\|a\\| = \\sqrt{ \\sum a_i^2 } $ es la [norma vectorial](https://es.wikipedia.org/wiki/Norma_vectorial) de $ a $.\n",
    "#### Podemos reorganizar los términos para despejar el coseno y comprobar que es el producto normalizado de dos vectores. Para nuestro modelo de vector, el cálculo del producto escalar y de las normas son funciones simples de la representación de un documento como una bag-of-words. Así pues estamos en codiciones de calcular la similitud:\n",
    "#### $$ similarity = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} $$\n",
    "#### Dejando a un lado el álgebra, la interpretación geométrica es más intituitiva. El ángulo entre dos vectores de documentos es pequeña si tienen muchos tokens en común porque están apuntando más o menos en la misma direccion con lo que el conseno del ángulo que formen será grande, por otro lado si el ángulo es grande (tienen pocas palabras en común) su coseno será pequeño.\n",
    "[cosine]: https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3a) Implementar los componentes de la función `cosineSimilarity`**\n",
    "#### Implement the components of a `cosineSimilarity` function.\n",
    "#### Usa las funciones `tokenize` y`tfidf` y los pesos IDF de la parte 2 para extraer tokens y asignarles pesos\n",
    "\n",
    "#### Los pasos son los siguientes:\n",
    "* #### Definir una función `dotprod` que tome dos diccionarios de Python y devuelva el producto escalar (dot product) entre ellos donde dot product se define como la suma de los productos de los valores para los tokens que aparezcan en *ambos* diccionarios\n",
    "* #### Definir una función `norm` que devuelva la raíz cuadrada del dot product de un diccionario consigo misma\n",
    "* #### Definir una función `cossim` que devuelva el dot product de dos diccionario dividido por la norma del primer diccionario y multiplicado por la norma del segundo diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "import math\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Calcula dot product\n",
    "    Args:\n",
    "        a (dictionary): primer diccionario de registro a valor\n",
    "        b (dictionary): segundo diccionario de registro a valor\n",
    "    Returns:\n",
    "        dotProd: Resultado del dot producto con los dos diccionarios de entrada\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Calcula la raíz cuadrada del dot product\n",
    "    Args:\n",
    "        a (dictionary): un diccionario de registro a valor\n",
    "    Returns:\n",
    "        norm: un diccionario de tokens a sus valores TF\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "def cossim(a, b):\n",
    "    \"\"\" Calcula la cosine similarity\n",
    "    Args:\n",
    "        a (dictionary): primer diccionario de registro a valor\n",
    "        b (dictionary): segundo diccionario de registro a valor\n",
    "    Returns:\n",
    "        cossim: dot product de dos diccionarios divido por la norma del primer diccionario multiplicado por la\n",
    "        norma del segundo\n",
    "    \"\"\"\n",
    "    return <RELLENA>\n",
    "\n",
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "print dp, nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implementar los componentes de la función cosineSimilarity (3a)\n",
    "Test.assertEquals(dp, 102, 'dp incorrecto')\n",
    "Test.assertTrue(abs(nm - 6.16441400297) < 0.0000001, 'nm incorrrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3b) Implementar la función `cosineSimilarity`**\n",
    "#### Implementar la función `cosineSimilarity(string1, string2, idfsDictionary)` que toma dos strings y un diccionario de pesos IDF y calcula su cosine similarity.\n",
    "#### Los pasos a realizar son:\n",
    "* #### Aplicar la función `tfidf` a las strings 1 y dos tokenizadas usando el diccionario de pesos IDF\n",
    "* #### Calcular y devolver la función `cossim` aplicada a los resultados de las dos funciones `tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    \"\"\" Calcula la cosine similarity entre dos strings\n",
    "    Args:\n",
    "        string1 (str): primera string\n",
    "        string2 (str): segunda string\n",
    "        idfsDictionary (dictionary):un diccionario de valores IDF\n",
    "    Returns:\n",
    "        cossim: valor cosine similarity \n",
    "    \"\"\"\n",
    "    w1 = tfidf(<RELLENA>)\n",
    "    w2 = tfidf(<RELLENA>)\n",
    "    return cossim(w1, w2)\n",
    "\n",
    "cossimAdobe = cosineSimilarity('Adobe Photoshop',\n",
    "                               'Adobe Illustrator',\n",
    "                               idfsSmallWeights)\n",
    "\n",
    "print cossimAdobe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Implementar la función cosineSimilarity (3b)\n",
    "Test.assertTrue(abs(cossimAdobe - 0.0577243382163) < 0.0000001, 'cossimAdobe incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3c) Realizar la resolución de entidades**\n",
    "#### Por último ¡Ya podemos realizar la resolución de entidades!\n",
    "#### Para *cada* registro de producto en el dataset pequeño de Google usaremos la función `cosineSimilarity` para calcular su similitud con cada registro de el dataset pequeño de Amazon, entonces construiremos un diccionario de mapeo de tuplas `(Google URL, Amazon ID)` con valores de similitud entre 0 y 1.\n",
    "#### Haremos este cálculo de dos maneras diferentes primero sin variables de broadcast y despúes usándolas.\n",
    "#### Los pasos a realizar son:\n",
    "* #### Crear un RDD combinación del dataset pequeño de Google y el de Amazon que tenga como elementos todos los pares de elementos con la forma: `[ ((Google URL1, Google String1), (Amazon ID1, Amazon String1)), ((Google URL1, Google String1), (Amazon ID2, Amazon String2)), ((Google URL2, Google String2), (Amazon ID1, Amazon String1)), ... ]`\n",
    "* #### Definir una función worker que dado un elemento del RDD anterior calcule cosineSimlarity para los dos registros en el elemento\n",
    "* #### Aplicar la función a cada elemento del RDD\n",
    "####  Calcula la similitud entre el registro de Amazon `b000o24l3q` y el registro de Google  `http://www.google.com/base/feeds/snippets/17242822440574356561`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "crossSmall = (googleSmall\n",
    "              .<RELLENA>\n",
    "              .cache())\n",
    "\n",
    "def computeSimilarity(record):\n",
    "    \"\"\" Calcula la similutud en una combinación de registros\n",
    "    Args:\n",
    "        record: un par, (registro google, registro amazon)\n",
    "    Returns:\n",
    "        tupla: una tupla, (google URL, amazon ID, valor de cosine similarity)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = <RELLENA>\n",
    "    amazonID = <RELLENA>\n",
    "    googleValue = <RELLENA>\n",
    "    amazonValue = <RELLENA>\n",
    "    cs = cosineSimilarity(<RELLENA>, idfsSmallWeights)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "similarities = (crossSmall\n",
    "                .<RELLENA>\n",
    "                .cache())\n",
    "\n",
    "def similar(amazonID, googleURL):\n",
    "    \"\"\" devuleve el valor de similitud\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: valor de cosine similarity\n",
    "    \"\"\"\n",
    "    return (similarities\n",
    "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
    "            .collect()[0][2])\n",
    "\n",
    "similarityAmazonGoogle = similar('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print 'la similitud es de is %s.' % similarityAmazonGoogle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Realizar la ER (3c)\n",
    "Test.assertTrue(abs(similarityAmazonGoogle - 0.000303171940451) < 0.0000001,\n",
    "                'similarityAmazonGoogle incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3d) Realizar la ER con variables de Broadcast**\n",
    "#### La solución de (3c) funciona bien para datasets pequeños pero requiere que Spark (automáticamente) mande la variable `idfsSmallWeights` a todos los workers. Si no hiciesemos  `cache()` en similarities,  se tendría que recrear cada ver que se llamase a  a `similar()` lo que haría que  `idfsSmallWeights` se enviase cada vez.\n",
    "#### Es mejor utilizar una variable de broadcast, definimos esta variable en el driver y nos podemos referir a ella en cada worker. Spark guarda la variable en cada uno de ellos y sólo la tenemos que enviar una vez.\n",
    "#### Pasos a realizar:\n",
    "* #### Definir una función `computeSimilarityBroadcast` que dado un elemento del RDD combinado calcule la similitud del coseno para dos los dos registros de cada elemento. Será la misma función `computeSimilarity` de (3c) excepto que utiliza una función de broadcast.\n",
    "* #### Aplicar la función worker a cada elemento del RDD\n",
    "#### Calcula de nuevo la similitud entre el registro Amazon `b000o24l3q` y el registro Google `http://www.google.com/base/feeds/snippets/17242822440574356561`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def computeSimilarityBroadcast(record):\n",
    "    \"\"\" Calcula la similitud en un registro combinado utilizando una variable de Broadcast\n",
    "    Args:\n",
    "        record: un par (google record, amazon record)\n",
    "    Returns:\n",
    "        tupla: una tupla (google URL, amazon ID, valor cosine similarity)\n",
    "    \"\"\"\n",
    "    googleRec = record[0]\n",
    "    amazonRec = record[1]\n",
    "    googleURL = <RELLENA>\n",
    "    amazonID = <RELLENA>\n",
    "    googleValue = <RELLENA>\n",
    "    amazonValue = <RELLENA>\n",
    "    cs = cosineSimilarity(<RELLENA>, idfsSmallBroadcast.value)\n",
    "    return (googleURL, amazonID, cs)\n",
    "\n",
    "idfsSmallBroadcast = sc.broadcast(idfsSmallWeights)\n",
    "similaritiesBroadcast = (crossSmall\n",
    "                         .<RELLENA>\n",
    "                         .cache())\n",
    "\n",
    "def similarBroadcast(amazonID, googleURL):\n",
    "    \"\"\" Devuelve un valor de similitud calculado utilizando una variable de broadcast\n",
    "    Args:\n",
    "        amazonID: amazon ID\n",
    "        googleURL: google URL\n",
    "    Returns:\n",
    "        similar: valor de cosine similarity\n",
    "    \"\"\"\n",
    "    return (similaritiesBroadcast\n",
    "            .filter(lambda record: (record[0] == googleURL and record[1] == amazonID))\n",
    "            .collect()[0][2])\n",
    "\n",
    "similarityAmazonGoogleBroadcast = similarBroadcast('b000o24l3q', 'http://www.google.com/base/feeds/snippets/17242822440574356561')\n",
    "print 'La similitud es %s.' % similarityAmazonGoogleBroadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Realiza ER con variables de Broadcast (3d)\n",
    "from pyspark import Broadcast\n",
    "Test.assertTrue(isinstance(idfsSmallBroadcast, Broadcast), 'idfsSmallBroadcast incorrecto')\n",
    "Test.assertEquals(len(idfsSmallBroadcast.value), 4772, 'valor idfsSmallBroadcast incorrecto')\n",
    "Test.assertTrue(abs(similarityAmazonGoogleBroadcast - 0.000303171940451) < 0.0000001,\n",
    "                'similarityAmazonGoogle incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(3e) Realizar la evaluación Gold Standard**\n",
    "#### Primero cargaremos los datos \"gold standard\" y los utilizaremos para resolver algunas cuestiones. Leeremos y parsearemos las líneas de los datos Gold Standard que tienen el siguiente formato: \"Amazon Product ID\",\"Google URL\". El RDD resultado tendrá elementos de la forma (\"AmazonID GoogleURL\", 'gold', 1 ó 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GOLDFILE_PATTERN = '^(.+),(.+)'\n",
    "\n",
    "# Parsea cada línea de un fichero de datos utilizado el patrón de expresión regular especificado\n",
    "def parse_goldfile_line(goldfile_line):\n",
    "    \"\"\" Parsea una línea de fichero de datos 'golden standard'\n",
    "    Args:\n",
    "        goldfile_line: una línea de datos\n",
    "    Returns:\n",
    "        tupla: ((key, 'gold', 1 en caso de éxito,  0 si no))\n",
    "    \"\"\"\n",
    "    match = re.search(GOLDFILE_PATTERN, goldfile_line)\n",
    "    if match is None:\n",
    "        print 'línea de goldfile inválida: %s' % goldfile_line\n",
    "        return (goldfile_line, -1)\n",
    "    elif match.group(1) == '\"idAmazon\"':\n",
    "        print 'línea de cabecera: %s' % goldfile_line\n",
    "        return (goldfile_line, 0)\n",
    "    else:\n",
    "        key = '%s %s' % (removeQuotes(match.group(1)), removeQuotes(match.group(2)))\n",
    "        return ((key, 'gold'), 1)\n",
    "\n",
    "goldfile = os.path.join(baseDir, inputPath, GOLD_STANDARD_PATH)\n",
    "gsRaw = (sc\n",
    "         .textFile(goldfile)\n",
    "         .map(parse_goldfile_line)\n",
    "         .cache())\n",
    "\n",
    "gsFailed = (gsRaw\n",
    "            .filter(lambda s: s[1] == -1)\n",
    "            .map(lambda s: s[0]))\n",
    "for line in gsFailed.take(10):\n",
    "    print 'línea del goldfile no válida: %s' % line\n",
    "\n",
    "goldStandard = (gsRaw\n",
    "                .filter(lambda s: s[1] == 1)\n",
    "                .map(lambda s: s[0])\n",
    "                .cache())\n",
    "\n",
    "print 'Leidas %d líneas, parseadas con éxito %d líneas, falladas al parsear %d líneas' % (gsRaw.count(),\n",
    "                                                                                 goldStandard.count(),\n",
    "                                                                                 gsFailed.count())\n",
    "assert (gsFailed.count() == 0)\n",
    "assert (gsRaw.count() == (goldStandard.count() + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando los datos \"gold standard\" podemos responder a las siguientes preguntas:\n",
    "* #### ¿Cuántos duplicados reales hay en los datasets pequeños?\n",
    "* #### ¿Cúal es la media de similitud para los duplicados reales?\n",
    "* #### ¿Y de los no duplicados?\n",
    "#### Los pasos a realizar son:\n",
    "* #### Crea un nuevo `sims` RDD a partir de `similaritiesBroadcast` RDD, donde cada elemento sea un par de la forma (\"AmazonID GoogleURL\", cosineSimilarityScore). Un ejemplo de `sims` podría ser: ('b000bi7uqs http://www.google.com/base/feeds/snippets/18403148885652932189', 0.40202896125621296)\n",
    "* #### Haz la composición de  `sims` RDD con el `goldStandard` RDD para crear un nuevo `trueDupsRDD` RDD que tiene el valor de similitud del coseno para los pares \"AmazonID GoogleURL\" que aparezcan tanto en `sims` RDD como en `goldStandard` RDD. Pista: Puedes usar la transformación join().\n",
    "\n",
    "* #### Cuenta el número de pares duplicado reales en `trueDupsRDD` dataset\n",
    "* #### Calcula la media de similitud para los duplicados reales en `trueDupsRDD` dataset. Utiliza `float` en los cálculos\n",
    "* #### Crea un nuevo RDD `nonDupsRDD` que tenga los valores de similitud del coseno para los pares \"AmazonID GoogleURL\"  del RDD `similaritiesBroadcast` que  **NO** aparezcan ni en *sims* RDD ni en el gold Standard RDD.\n",
    "* #### Calcula la similitud media para los no duplicados en el último dataset. Recuerda usar `float` en los cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "sims = similaritiesBroadcast.<RELLENA>)\n",
    "\n",
    "trueDupsRDD = (sims\n",
    "               .<RELLENA>)\n",
    "trueDupsCount = trueDupsRDD.<RELLENA>\n",
    "avgSimDups = <RELLENA>\n",
    "\n",
    "nonDupsRDD = (sims\n",
    "              .<RELLENA>)\n",
    "avgSimNon = <RELLENA>\n",
    "\n",
    "print 'Hay %s duplicados reales.' % trueDupsCount\n",
    "print 'La media de los duplicados reales es %s.' % avgSimDups\n",
    "print 'y ara los no duplicados la media es %s.' % avgSimNon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Realizar una evaluación Gold Standard (3e)\n",
    "Test.assertEquals(trueDupsCount, 146, 'trueDupsCount incorrecta')\n",
    "Test.assertTrue(abs(avgSimDups - 0.264332573435) < 0.0000001, 'avgSimDups incorrecta')\n",
    "Test.assertTrue(abs(avgSimNon - 0.00123476304656) < 0.0000001, 'avgSimNon incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4: ER escalable**\n",
    "#### La función de similitud construida anteriormente sólo la podemos utilizar para pequeños datasets ya que su complejidad es cuadrática y no es práctica incluso para datasets de tamaño medianoo. En esta parte implementaremos un algoritmo más escalable y lo usaremos para la resolución de entidades en todo el dataset.\n",
    "### Índices invertidos\n",
    "#### Para mejorar el algoritmo ER empezaremos analizando su tiempo de ejecución. El algoritmo es cuadrático de dos maneras, por un lado hace un montón de cálculos redundantes de los tokens y los pesos ya que cada registro se reprocesa cada vez que se compara. Por otro lado se hacen de manera cuadrática las comparaciones de tokens entre registros.\n",
    "#### El primer problema se puede eliminar realizando un precálculo y almacenando los resultados en tablas de look-up, El segundo problema es un poco más dificil. En el peor de los casos, cada token en cada registro en un dataset existe en cada registro del otro dataset por lo que cada token hará una contribución distinta de cero a la similitud del coseno. En ese caso la comparación de tokens es inevitablemente cuadrática.\n",
    "#### Pero en realidad la mayoría de los registros no tiene nada (o muy poco) en común, por otra parte lo normal será que un registro en un dataset tenga como mucho un duplicado en el otro (asumiendo que cada dataset ha sido deduplicado contra si mismo previamente) en este caso la salida será lineal con respecto al tamaño de la entrada y podemos esperar a alcanzar un tiempo de ejecución lineal.\n",
    "#### Un [**Índice invertido**](https://en.wikipedia.org/wiki/Inverted_index) es una estructura de datos que nos permitirá evitar hacer comparaciones cuadráticas de datos. Mapea cada token en el dataset con la lista de documentos que contiene ese token así en vez de comparar registro a registro cada token con otro para ver se hay coincidencia usaremos índices invertidos para hacer un *look up* de los registros para un token particular.\n",
    "> #### **Nota sobre la terminología**: En búsqueda de textos un índice *directo* (*foward*) mapea los documentos en un dataset con los tokens que contienen. Un índice *invertido* (*inverted*) realiza el mapeo inverso.\n",
    "> #### **Nota**: Para esta sección usaremos los datasets completos de Google y Amazon no las muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4a) Tokenizar el dataset completo**\n",
    "#### Tokenize each of the two full datasets for Google and Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "amazonFullRecToToken = amazon.<RELLENA>\n",
    "googleFullRecToToken = google.<RELLENA>\n",
    "print 'El dataset completo de Amazon tiene %s productos, El dataset completo de Google tiene %s productos' % \n",
    "                                                                                    (amazonFullRecToToken.count(),\n",
    "                                                                                    googleFullRecToToken.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST tokenizar el dataset completo (4a)\n",
    "Test.assertEquals(amazonFullRecToToken.count(), 1363, 'amazonFullRecToToken.count() incorrecta')\n",
    "Test.assertEquals(googleFullRecToToken.count(), 3226, 'googleFullRecToToken.count() incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4b) Calcular IDFs y TF-IDFs para los dataset completos**\n",
    "#### Reusaremos el código anterior para calcular los pesos IDF para todo el dataset combinado\n",
    "#### Pasos a realizar:\n",
    "* #### Crear un nuevo `fullCorpusRDD` que contenga los tokens de los datasets completos de Amazon y Google.\n",
    "* #### Aplicar la función `idfs` a `fullCorpusRDD`\n",
    "* #### Crear una variable de broadcst que contenga un diccionario de los pesos IDF para todo el dataset. \n",
    "* #### Para cada dataset completo de Amazon y Google crear RDDs de pesos que mapeen IDs/URLs a vectores de tokens TF-IDF ponderados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "fullCorpusRDD = <RELLENA>\n",
    "idfsFull = idfs(fullCorpusRDD)\n",
    "idfsFullCount = idfsFull.count()\n",
    "print 'Hay %s tokens únicos en los dataset completos.' % idfsFullCount\n",
    "\n",
    "# Recalcular IDFs para todo el dataset\n",
    "idfsFullWeights = <RELLENA>\n",
    "idfsFullBroadcast = <RELLENA>\n",
    "\n",
    "# Precalcular los pesos TF-IDF. Construir el mapeo entre el ID del registro al vector de pesos\n",
    "amazonWeightsRDD = <RELLENA>\n",
    "googleWeightsRDD = <RELLENA>\n",
    "print 'Hay %s pesos de Amazon y %s pesos de Google.' % (amazonWeightsRDD.count(),\n",
    "                                                              googleWeightsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Calcular IDFs y TF-IDFs para todo el dataset (4b)\n",
    "Test.assertEquals(idfsFullCount, 17078, 'idfsFullCount incorrecta')\n",
    "Test.assertEquals(amazonWeightsRDD.count(), 1363,  amazonWeightsRDD.count() incorrecta')\n",
    "Test.assertEquals(googleWeightsRDD.count(), 3226, 'googleWeightsRDD.count() incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4c) Calcular las normas para los pesos de todo el dataset**\n",
    "#### Reutilizaremos el código anterior para calcular las normas de los pesos IDF para todo el dataset combinado.\n",
    "#### Los pasos a realizar son:\n",
    "* #### Crear dos colecciones, una por cada uno de los datasets completos de Amazon y Google, donde los IDS/URLs mapean la norma de sus vectores de pesos de tokens TF-IDF.\n",
    "* #### Convertir cada colección en una variable de broadcast que contenga un dicionario de la norma de los pesos IDF para todo el dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "amazonNorms = amazonWeightsRDD.<RELLENA>\n",
    "amazonNormsBroadcast = <RELLENA>\n",
    "googleNorms = googleWeightsRDD.<RELLENA>\n",
    "googleNormsBroadcast = <RELLENA>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Calcula las Normas para los pesos de todo el dataset (4c)\n",
    "Test.assertTrue(isinstance(amazonNormsBroadcast, Broadcast), 'amazonNormsBroadcast incorrecta')\n",
    "Test.assertEquals(len(amazonNormsBroadcast.value), 1363, 'amazonNormsBroadcast.value incorrecto')\n",
    "Test.assertTrue(isinstance(googleNormsBroadcast, Broadcast), 'googleNormsBroadcast incorrecta')\n",
    "Test.assertEquals(len(googleNormsBroadcast.value), 3226, 'googleNormsBroadcast.value incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4d) Crear índices invertidos para ambas fuentes de datos**\n",
    "#### Los pasos a realizar son:\n",
    "* #### Crear una función de inversión que dado un par (ID/URL, vector de pesos de tokens TF-IDF) devuelva una lista de pares (token, ID/URL). Hay que recordar que el vector de pesos de tokens TF-IDF es un diccionario Python con keys que son los tokens y valores que son los pesos.\n",
    "* #### Usar la función de inversión para convertir los dataset de Amazon y Google de vectores de pesos de tokens TF-IDF en dos RDDs donde cada elemento es un par de un token y un ID/URL que contiene ese token. Estos son los índices invertidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def invert(record):\n",
    "    \"\"\" Invierte (ID, tokens) en una lista de (token, ID)\n",
    "    Args:\n",
    "        record: un park, (ID, token vector)\n",
    "    Returns:\n",
    "        pairs: una lista de pares de token a ID\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "    return (pairs)\n",
    "\n",
    "amazonInvPairsRDD = (amazonWeightsRDD\n",
    "                    .<RELLENA>\n",
    "                    .cache())\n",
    "\n",
    "googleInvPairsRDD = (googleWeightsRDD\n",
    "                    .<RELLENA>\n",
    "                    .cache())\n",
    "\n",
    "print 'Hay %s pares invertidos de Amazon y %s pares invertidos de Google.' % (amazonInvPairsRDD.count(),\n",
    "                                                                            googleInvPairsRDD.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Crear índices invertidos para todo el dataset (4d)\n",
    "invertedPair = invert((1, {'foo': 2}))\n",
    "Test.assertEquals(invertedPair[0][1], 1, 'resultado de inversión incorrecto')\n",
    "Test.assertEquals(amazonInvPairsRDD.count(), 111387, 'amazonInvPairsRDD.count() incorrecta')\n",
    "Test.assertEquals(googleInvPairsRDD.count(), 77678, 'googleInvPairsRDD.count() incorrecta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4e) Identificar tokens comunes en todo el dataset**\n",
    "#### Ahora podemos realizar de manera eficiente la ER en todo el dataset. Implementando el siguiente algoritmo para construir un RDD que mapee un par (ID, URL) a  una lista de tokens que tienen en común: \n",
    "* #### Usando los dos índices invertidos (RDDs donde cada elemento es un par de un token y el ID o URL que contiene ese token), crea un nuevo RDD que contenga sólo los tokens que aparezcan ambos datasets, los elementos de este RDD serán de la forma (token, iterable (ID, URL)).\n",
    "\n",
    "* #### Necesitamos un mapeo de (ID, URL) a token asi que crearemos una función que haga el swap de los elementos del RDD anterior para tener un nuevo RDD de la forma ((ID, URL), token).\n",
    "* #### Por último crear un RDD que contenga pares (ID, URL)  con todos los tokens que estos pares tengan en común."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def swap(record):\n",
    "    \"\"\" Swap (token, (ID, URL)) a ((ID, URL), token)\n",
    "    Args:\n",
    "        record: un par, (token, (ID, URL))\n",
    "    Returns:\n",
    "        pair: ((ID, URL), token)\n",
    "    \"\"\"\n",
    "    token = <RELLENA>\n",
    "    keys = <RELLENA>\n",
    "    return (keys, token)\n",
    "\n",
    "commonTokens = (amazonInvPairsRDD\n",
    "                .<RELLENA>\n",
    "                .cache())\n",
    "\n",
    "print 'Encontrados %d tokens en común' % commonTokens.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Identificar los tokens en comun en el dataset (4e)\n",
    "Test.assertEquals(commonTokens.count(), 2441100, 'incorrect commonTokens.count()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(4f) Identificar tokens comunes en todo el dataset cont.**\n",
    "#### Utiliza las estructuras de datos de la parte **(4a)** y **(4e)** para construir un diccionario que mapee pares de registros a valores de similitud del coseno.  \n",
    "#### Los pasos a realizar son:\n",
    "* #### Crear dos diccionarios de broadcast para los RDDs amazonWeights y googleWeights\n",
    "* #### Crear una función `fastCosinesSimilarity` que tome un registros del tipo ((Amazon ID, Google URL), lista de tokens) y calcule la suma para cada uno de los tokens en la lista de tokens como resultado de multiplicar el peso del token de amazon por el peso del token Google. La suma se dividirá entre la norma de la URL de Google y luego entre la norma del ID de Amazon. La función debe devolver el valor como un par en la que la clave sea (Amazon ID, Google URL). *Asegúrate de usar las variables de broadcast creadas para pesos y normas*\n",
    "\n",
    "* #### Aplica la función `fastCosinesSimilarity` a los tokens de todo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "amazonWeightsBroadcast = <RELLENA>\n",
    "googleWeightsBroadcast = <RELLENA>\n",
    "\n",
    "def fastCosineSimilarity(record):\n",
    "    \"\"\" Calcula Cosine Similarity usando variables de Broadcast\n",
    "    Args:\n",
    "        record: ((ID, URL), token)\n",
    "    Returns:\n",
    "        pair: ((ID, URL), valor de cosine similarity )\n",
    "    \"\"\"\n",
    "    amazonRec = <RELLENA>\n",
    "    googleRec = <RELLENA>\n",
    "    tokens = <RELLENA>\n",
    "    s = <RELLENA>\n",
    "    value = <RELLENA>\n",
    "    key = (amazonRec, googleRec)\n",
    "    return (key, value)\n",
    "\n",
    "similaritiesFullRDD = (commonTokens\n",
    "                       .<RELLENA>\n",
    "                       .cache())\n",
    "\n",
    "print similaritiesFullRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Identificar tokens comunes en todo el dataset cont. (4f)\n",
    "similarityTest = similaritiesFullRDD.filter(lambda ((aID, gURL), cs): aID == 'b00005lzly' and gURL == 'http://www.google.com/base/feeds/snippets/13823221823254120257').collect()\n",
    "Test.assertEquals(len(similarityTest), 1, 'len(similarityTest) incorrecta')\n",
    "Test.assertTrue(abs(similarityTest[0][1] - 4.286548414e-06) < 0.000000000001, 'similarityTest fastCosineSimilarity incorrecto')\n",
    "Test.assertEquals(similaritiesFullRDD.count(), 2441100, 'similaritiesFullRDD.count() incorrecto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 5: Análisis**\n",
    "#### Ahora tenemos una lista fiable de pares de registros con su similitudes pero necesitamos una manera de decidir si dos registros son duplicados o no. La forma más simple es elegir un **threshold**. Los pares cuya similitud esté por encima de ese threshold son considerados duplicados y los que estén por debajo serán considerados distintos.\n",
    "\n",
    "#### Para decidir donde situar el thresold necesitamos entender que clase de errores resultarán a diferentes niveles. Si seleccionamos un threshold muy bajo obtendremos **falsos positivos** esto es pares de registros que consideraremos duplicados cuando realmente no lo son. Si lo ponemos demasiado alto obtendremos **falsos negativos** esto es pares de registros que realmente son duplicados pero que perderemos.\n",
    "\n",
    "#### Los algoritmos ER se evaluan con métricas de la [teoría de la búsqueda y recuperación de información](https://es.wikipedia.org/wiki/B%C3%BAsqueda_y_recuperaci%C3%B3n_de_informaci%C3%B3n) llamadas **precisión (precission)** y **Exhaustividad (recall)**. La precisión pregunta de todos los pares de registros marcados como duplicados la fracción de los que realmente son duplicados. Recall pregunta de entre todos los verdaderos duplicados en los datos la fracción que realmente se encontró con éxito. Como con los falsos positivos y falsos negativos hay una compensación entre entre precisión y racall. Una tercera métrica **medida F (F-measure)** toma la [media armónica](https://es.wikipedia.org/wiki/Media_arm%C3%B3nica) de precisión y recall para medir la bondad total en un único valor:\n",
    "#### $$ Fmeasure = 2 \\frac{precision * recall}{precision + recall} $$\n",
    "> #### **Nota**: En esta parte usaremos el fichero \"gold standard\" para comprobar los verdaderos duplicados y los resultados de la parte 4.\n",
    "> #### **Nota**: ¡Eh! Ya no vas a escribir más código. Ejecuta cada celda y quedate con la idea general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5a) Contando Verdaderos Positivos, Falsos Positivos y Falsos Negativos**\n",
    "#### Necesitamos funciones que cuenten verdaderos positivos (verdaderos duplicados por encima del threshold), falsos positivos y falsos negativos: \n",
    "* #### Creamos `simsFullRDD` desde `similaritiesFullRDD` que tendrá elementos de la forma ((Amazon ID, Google URL), similarity score)\n",
    "* #### A partir de este RDD, creamos otro sólo con los similarity scores\n",
    "* #### Para buscar verdaderos duplicados hacemos un left outer join con el `goldStandard` RDD y `simsFullRDD` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Crear RDD de ((Amazon ID, Google URL), similarity score)\n",
    "simsFullRDD = similaritiesFullRDD.map(lambda x: (\"%s %s\" % (x[0][0], x[0][1]), x[1]))\n",
    "assert (simsFullRDD.count() == 2441100)\n",
    "\n",
    "# Crear RDD sólo con similarity scores\n",
    "simsFullValuesRDD = (simsFullRDD\n",
    "                     .map(lambda x: x[1])\n",
    "                     .cache())\n",
    "assert (simsFullValuesRDD.count() == 2441100)\n",
    "\n",
    "# Buscamos todos los similarity scores para verdaderos positivos\n",
    "\n",
    "# Esta función devolverá el score de similitud para registros que estan en el gol standard y en simsFulRDD \n",
    "#(Verdaderos positivos) y devolverá 0 para registros que están en el gold standar pero no estan en \n",
    "#simsFullRDD (Falsos Negativos)\n",
    "def gs_value(record):\n",
    "    if (record[1][1] is None):\n",
    "        return 0\n",
    "    else:\n",
    "        return record[1][1]\n",
    "\n",
    "# Join del gold standard y simsFullRDD y luego extrae los similarities scores utilizado la función anterior \n",
    "trueDupSimsRDD = (goldStandard\n",
    "                  .leftOuterJoin(simsFullRDD)\n",
    "                  .map(gs_value)\n",
    "                  .cache())\n",
    "print 'Hay %s duplicados verdaderos.' % trueDupSimsRDD.count()\n",
    "assert(trueDupSimsRDD.count() == 1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El siguiente paso es elegir un threshold entre 0 y 1 para contar los Verdaderos Positivos (verdaderos duplicados por encima del threshold). Queremos explorar varios thresholds diferentes. Para hacer esto dividimos el espacio de thresholds en 100 contenedores y hacemos lo siguiente:\n",
    "\n",
    "* #### Utilizamos acumuladores de Spark para implementar nuestra función de conteo. Definimos un tipo de acumulador `VectorAccumulatorParam`, junto con funciones para inicializar el vector del acumulador a cero y sumar dos vectores. Usamos el operador += porque solo se puede sumar en un acumulador.\n",
    "* #### Creamos una función auxiliar para tener una lista con una entrada (bit) puesto a un valor y los otros a 0.\n",
    "* #### Creamos 101 contenedores para los 100 valores de threshold entre 0 y 1.\n",
    "* #### Ahora por cada similarity score, calculamos los falsos positivos. Hacemos esto sumando cada similarity score al contenedor apropiado del vector. Luego borramos los verdaderos positivos del vector usando los datos de gold standard\n",
    "* #### Definimos funciones para calcular falsos positivos y negativos y verdaderos positivos para un threshold dado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/1.1.0/api/python/pyspark.accumulators-module.html\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    # Inicializar el vector de acumulacion a 0\n",
    "    def zero(self, value):\n",
    "        return [0] * len(value)\n",
    "\n",
    "    # suma dos variables del vector de acumulacion\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in xrange(len(val1)):\n",
    "            val1[i] += val2[i]\n",
    "        return val1\n",
    "\n",
    "# devuelve una lista con la entrada x puesta a un valor y el resto de las entradas a 0\n",
    "def set_bit(x, value, length):\n",
    "    bits = []\n",
    "    for y in xrange(length):\n",
    "        if (x == y):\n",
    "          bits.append(value)\n",
    "        else:\n",
    "          bits.append(0)\n",
    "    return bits\n",
    "\n",
    "# Pre-bin cuenta de falsos positivos para diferentes rangos de threshold \n",
    "BINS = 101\n",
    "nthresholds = 100\n",
    "def bin(similarity):\n",
    "    return int(similarity * nthresholds)\n",
    "\n",
    "# fpCounts[i] = numero de entradas (posibles falsos positivos) donde donde  bin(similarity) == i\n",
    "zeros = [0] * BINS\n",
    "fpCounts = sc.accumulator(zeros, VectorAccumulatorParam())\n",
    "\n",
    "def add_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, 1, BINS)\n",
    "\n",
    "simsFullValuesRDD.foreach(add_element)\n",
    "\n",
    "# Borrar los positivos verdaderos de la cuenta de FP\n",
    "def sub_element(score):\n",
    "    global fpCounts\n",
    "    b = bin(score)\n",
    "    fpCounts += set_bit(b, -1, BINS)\n",
    "\n",
    "trueDupSimsRDD.foreach(sub_element)\n",
    "\n",
    "def falsepos(threshold):\n",
    "    fpList = fpCounts.value\n",
    "    return sum([fpList[b] for b in range(0, BINS) if float(b) / nthresholds >= threshold])\n",
    "\n",
    "def falseneg(threshold):\n",
    "    return trueDupSimsRDD.filter(lambda x: x < threshold).count()\n",
    "\n",
    "def truepos(threshold):\n",
    "    return trueDupSimsRDD.count() - falsenegDict[threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5b) Precisión, Recall, y F-measures**\n",
    "#### Definimos funciones para poder calcular  [Precision][precision-recall], [Recall][precision-recall], y [F-measure][f-measure] como funciones con el threshold como parámetro:\n",
    "* #### Precision = true-positives / (true-positives + false-positives)\n",
    "* #### Recall = true-positives / (true-positives + false-negatives)\n",
    "* #### F-measure = 2 x Recall x Precision / (Recall + Precision)\n",
    "[precision-recall]: https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "[f-measure]: https://en.wikipedia.org/wiki/Precision_and_recall#F-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision = true-positives / (true-positives + false-positives)\n",
    "# Recall = true-positives / (true-positives + false-negatives)\n",
    "# F-measure = 2 x Recall x Precision / (Recall + Precision)\n",
    "\n",
    "def precision(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falseposDict[threshold])\n",
    "\n",
    "def recall(threshold):\n",
    "    tp = trueposDict[threshold]\n",
    "    return float(tp) / (tp + falsenegDict[threshold])\n",
    "\n",
    "def fmeasure(threshold):\n",
    "    r = recall(threshold)\n",
    "    p = precision(threshold)\n",
    "    return 2 * r * p / (r + p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(5c) Plot de las líneas**\n",
    "#### Dibujamos las gráficas de precisión, recall y F-measure como una función del valor de threshold para thresholds entre 0.0 y 1.0. como se observa el mejor threshol está en torno a 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n",
    "falseposDict = dict([(t, falsepos(t)) for t in thresholds])\n",
    "falsenegDict = dict([(t, falseneg(t)) for t in thresholds])\n",
    "trueposDict = dict([(t, truepos(t)) for t in thresholds])\n",
    "\n",
    "precisions = [precision(t) for t in thresholds]\n",
    "recalls = [recall(t) for t in thresholds]\n",
    "fmeasures = [fmeasure(t) for t in thresholds]\n",
    "\n",
    "print precisions[0], fmeasures[0]\n",
    "assert (abs(precisions[0] - 0.000532546802671) < 0.0000001)\n",
    "assert (abs(fmeasures[0] - 0.00106452669505) < 0.0000001)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(thresholds, precisions)\n",
    "plt.plot(thresholds, recalls)\n",
    "plt.plot(thresholds, fmeasures)\n",
    "plt.legend(['Precision', 'Recall', 'F-measure'])\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
