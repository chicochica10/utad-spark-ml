{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ML Logo](https://raw.githubusercontent.com/chicochica10/utad-spark-ml/master/images/utad-spark-ml.1x_Banner_300.png)\n",
    "# ** Lab de Análisis de componentes principales**\n",
    "#### Este lab se adentra en el análisis exploratorio de datos del mundo de la neurociencia (no os asustéis, es más fácil de lo que parece) en concreto vamos a utilizar el análisis de componenentes principales (PCA) y la agregación basada en características. Los datos que usaremos son del laboratorio  [Ahrens](http://www.janelia.org/lab/ahrens-lab) y están almacenados en el siguiente [data repository](http://datasets.codeneuro.org).\n",
    "\n",
    "#### Los datos provienen del estudio de un pez transparente llamado [pez cebra](http://en.wikipedia.org/wiki/Zebrafish) al ser transparente es posible registrar su actividad neuronal aplicando la técnica [light-sheet microscopy](http://en.wikipedia.org/wiki/Light_sheet_fluorescence_microscopy).  En concreto vamos a trabajar con los datos neuronales del pez cebra al ser expuesto a distintos estímulos visuales móviles. Éstos estímulos inducen diferentes patrones en el cerebro y podemos utilizar análisis exploratorio para identificarlos.  Más información en  [\"Mapping brain activity at scale with cluster computing\"](http://thefreemanlab.com/work/papers/freeman-2014-nature-methods.pdf) \n",
    "\n",
    "#### En ete lab aprenderás a trabajar con PCA y comparar diferentes análisis exploratorios sobre los mismos datos para identificar que patrones neuronales son los que más resaltan.\n",
    "\n",
    "#### ** Este lab cubre: **\n",
    "+  ####*Parte 1:* Pasos de un PCA sobre un dataset de muestra\n",
    " + ####*Visualización 1:* Gausianas de dos dimensiones\n",
    "+  ####*Parte 2:* Escribir una función PCA y evaluación en el dataset de muestra\n",
    " + ####*Visualización 2:* Proyección PCA\n",
    " + ####*Visualización 3:* Datos tridimensionales\n",
    " + ####*Visualización 4:* Respresentación en 2D de datos 3D\n",
    "+  ####*Parte 3:* Parseo, Inspección y preprocesamiento de datos para PCA\n",
    " + ####*Visualización 5:* Intensidad de Pixel\n",
    " + ####*Visualización 6:* Datos Normalizados\n",
    " + ####*Visualización 7:* Los dos mejores componentes como imágenes\n",
    " + ####*Visualización 8:* Los dos mejores componentes como una única imagen\n",
    "+  ####*Parte 4:* Agregación basada en características seguida de PCA\n",
    " + ####*Visualización 9:* Los dos mejores componentes en el tiempo\n",
    " + ####*Visualización 10:* Los dos mejores componentes por dirección\n",
    " \n",
    "#### Puedes consultar el API de Spark en [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) y el de NumPy lo tienes en [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### **Parte 1: Pasos de un PCA sobre un dataset de muestra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 1: Gausianas de dos dimensiones**\n",
    "#### El análisis de componentes principales o PCA es una estrategia para la reducción de dimensiones. Para entender mejor PCA, trabajaremos con datos sintéticos generados mediante un muestreo de una [distribución bidemensional gausiana](http://en.wikipedia.org/wiki/Multivariate_normal_distribution).  Esta distribución toma como entrada la media y la varianza de cada dimensión y la covarianza entre las dos dimensiones.\n",
    " \n",
    "#### En las visualizaciones de abajo especificamos la media de cada dimensión a 50 y la varianza de cada dimensión la fijamos a 1. Exploraremos dos valores diferentes para la covarianza: 0 y 0.9. Cuando la covarianza es 0, las dos dimensiones no están correladas y por lo tanto los datos parecerán esféricos mientras que si la covarianza es 0.9 ambas dimensiones están fuerte (y positivamente) correladas y por lo tanto los datos no son esféricos. Como veremos en las partes 1 y 2, los datos no esféricos son susceptibles de una reducción dimensional vía PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\" Plantilla para generar la rejilla.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "def create2DGaussian(mn, sigma, cov, n):\n",
    "    \"\"\" puntos aleatorios de una gausiana bidimensional \"\"\"\n",
    "    np.random.seed(142)\n",
    "    return np.random.multivariate_normal(np.array([mn, mn]), np.array([[sigma, cov], [cov, sigma]]), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataRandom = create2DGaussian(mn=50, sigma=1, cov=0, n=100)\n",
    "\n",
    "# generación de la rejila y plot de datos\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45, 54.5), ax.set_ylim(45, 54.5)\n",
    "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataCorrelated = create2DGaussian(mn=50, sigma=1, cov=.9, n=100)\n",
    "\n",
    "# generación de la rejila y plot de datos\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1a) Interpretación de PCA**\n",
    "#### Se puede pensar en PCA en la manera de identificar las \"direcciones\" en las que los datos varían más. En un primer paso del PCA centramos nuestros datos. En nuestro dataset correlado primero calcularemos la media de cada característica (columna). Para cada observación modifica las características restando su media para crear un dataset centrado en la media (media cero) \n",
    "#### `correlatedData` es un RDD de arrays de NumPy esto nos permite realizar ciertas operaciones de una manera más sucinta, por ejemplo podemos sumar las columnas de nuestro dataset usando  `correlatedData.sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "correlatedData = sc.parallelize(dataCorrelated)\n",
    "\n",
    "meanCorrelated = <RELLENA>\n",
    "correlatedDataZeroMean = correlatedData.<RELLENA>\n",
    "\n",
    "print meanCorrelated\n",
    "print correlatedData.take(1)\n",
    "print correlatedDataZeroMean.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Interpretación de PCA (1a)\n",
    "from test_helper import Test\n",
    "Test.assertTrue(np.allclose(meanCorrelated, [49.95739037, 49.97180477]),\n",
    "                'valores incorrectos para meanCorrelated')\n",
    "Test.assertTrue(np.allclose(correlatedDataZeroMean.take(1)[0], [-0.28561917, 0.10351492]),\n",
    "                'valores incorrectos para correlatedDataZeroMean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1b) Matriz de covarianza de muestra**\n",
    "#### Ahora estamos listos para calcular la matriz de covarianza de muestra, si definimos  $\\scriptsize \\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ como la matriz de datos de media cero la matriz de muestra se define como: $$ \\mathbf{C}_{\\mathbf X} = \\frac{1}{n} \\mathbf{X}^\\top \\mathbf{X} \\,.$$  Para calcular esta matriz se realiza el producto externo (outer product) para cada punto de datos. Los datos son bidimensionales asi que la matriz de covarianza resultante debería ser una matriz de 2x2.\n",
    " \n",
    "#### Se puede usar [np.outer()](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html) para calcular el outer product de dos arrays NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# Calcula la matriz de covarianza usando productos externos y and correlatedDataZeroMean\n",
    "correlatedCov = <RELLENA>\n",
    "print correlatedCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Matriz de covariaza de muestra (1b)\n",
    "covResult = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "Test.assertTrue(np.allclose(covResult, correlatedCov), 'valor incorrecto para correlatedCov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1c) Función de covarianza**\n",
    "#### A continuación usaremos la expresión anterior para escribir una función que calcule la matriz de covarianza de muestra para un RDD arbitrario `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def estimateCovariance(data):\n",
    "    \"\"\" Calcular la matriz de covarianza para un rdd dado.\n",
    "\n",
    "    Nota:\n",
    "        El array de covarianza multi-dimensional debería calcurse usando outer prudcts. No olvides\n",
    "        Normalizar los datos restando primero la media.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray):  Un `RDD` de arrays de NumPy.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: un array multi-dimensional donde el número de filas y columnas son iguales a la longitud \n",
    "        de los arrays en el `RDD` de entrada.\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "\n",
    "correlatedCovAuto= estimateCovariance(correlatedData)\n",
    "print correlatedCovAuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Función de Covarianza (1c)\n",
    "correctCov = [[ 0.99558386,  0.90148989], [0.90148989, 1.08607497]]\n",
    "Test.assertTrue(np.allclose(correctCov, correlatedCovAuto),\n",
    "                'valor incorrecto para correlatedCovAuto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1d) Eigendescomposición**\n",
    "#### Ahora que hemos calculado la matriz de covarianza de muestra la podemos usar para encontrar las direcciones de variación máxima de la varianza en los datos, en concreto, podemos realizar una eigendescomposición de esta matriz para encontrar los autovalores y los autovectores.  Los  $\\scriptsize d $ autovectores de la matriz de covarianza nos dan las direcciones de máxima varianza y se les suelen llamar \"componentes principales\". Los autovalores asociados son las varianzas en esas direcciones. En particular el autovector que se corresponde con el mayor autovalor es la dirección de variación máxima (a veces se le llama el autovector principal o \"top\" egigenvector). La Eigendescomposición de una matriz $\\scriptsize d \\times d $ de covarianza tiene (grosso modo) una complejidad de ejecución cúbica con respecto a  $\\scriptsize d $.  Cuando $\\scriptsize d $ es relativamente pequeño (por ejemplo menos que unos pocos miles) podemos realizar la eigendescomposición localmente.\n",
    "\n",
    "#### Usar la función de `numpy.linalg` llamada [eigh](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigh.html) para realizar la eigendescomposición. A continuación ordena los autovectores en relación a sus correspondientes autovalores del más alto al más bajo, devolviendo una matriz donde las columnas son los autovectores (y la primera columna es el principal autovector). Puedes usar  [np.argsort](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy-argsort) para obtener los índices de los autovalores que se corresponden con el orden ascendente de los autovalores. Por último asigna la variable `topComponent` al autovector principal (componente principal) que es un vector  $\\scriptsize 2 $-dimensional  (array con dos valores).\n",
    "#### Nota: Los autovectores devueltos por  `eigh` están en columnas, no en filas, Por ejemplo el primer autovector de `eigVecs` se encontraría en la primera columna y se accedería a él usando `eigVecs[:,0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "from numpy.linalg import eigh\n",
    "\n",
    "# Calcula los autovalores y autovectores de correlatedCovAuto\n",
    "eigVals, eigVecs = <RELLENA>\n",
    "print 'eigenvalues: {0}'.format(eigVals)\n",
    "print '\\neigenvectors: \\n{0}'.format(eigVecs)\n",
    "\n",
    "# Usa np.argsort para encontrar el eigenvector principal basado en el mayor eigenvalue\n",
    "inds = np.argsort(<RELLENA>)\n",
    "topComponent = <RELLENA>\n",
    "print '\\nMayor de los componentes principales: {0}'.format(topComponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Eigendescomposición (1d)\n",
    "def checkBasis(vectors, correct):\n",
    "    return np.allclose(vectors, correct) or np.allclose(np.negative(vectors), correct)\n",
    "Test.assertTrue(checkBasis(topComponent, [0.68915649, 0.72461254]),\n",
    "                'valor incorrecto para topComponent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1e) Puntuaciones PCA**\n",
    "#### Acabamos de calcular el mayor de  los componentes principales para un dataset no esférico y bidimensional. Ahora vamos a usar este componente principal para obtener una representación unidimensional para los datos originales. Para calcular esta representación compacta que a veces se denomina \"puntuaciones PCA\" se realiza el dot product entre cada punto de los datos originales y el mayor de los componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# Usa topComponent y los datos de correlatedData para generar las puntuaciones PCA\n",
    "correlatedDataScores = <RELLENA>\n",
    "print 'tres primeros datos unidimensionales:\\n{0}'.format(np.asarray(correlatedDataScores.take(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST puntuaciones PCA (1e)\n",
    "firstThree = [70.51682806, 69.30622356, 71.13588168]\n",
    "Test.assertTrue(checkBasis(correlatedDataScores.take(3), firstThree),\n",
    "                'valor incorreto para correlatedDataScores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 2: Escribir una función PCA y evaluación en el dataset de muestra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Función de PCA**\n",
    "#### Ahora tenemos todos los ingredientes para escribir una función general de PCA. En lugar de trabajar con los componentes principales más altos nuestra función calculará los $\\scriptsize k$ componentes principales más altos y las principales puntuaciones para un dataset. Escribe la función general `pca` y ejecútala con `correlatedData` y $\\scriptsize k = 2$. Utiliza los resultados de las partes (1c), (1d) y (1e).\n",
    "\n",
    "#### Recuerda nuestra implementación es una estrategia razonable cuando $\\scriptsize d $ es pequeño pero hay algoritmos distribuidos más eficaces cuando $\\scriptsize d $ es grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def pca(data, k=2):\n",
    "    \"\"\" Calcula los `k` componentes principales más altos sus puntuaciones correspondientes\n",
    "    y todos los autovalores\n",
    "    \n",
    "    Note:\n",
    "        Todos los autovalores deberían devolverse ordenados de mayor a menor. `eigh` devuelve cada\n",
    "        autovector como una columna. Esta función debería también devolver los autovectores como columnas.\n",
    "\n",
    "    Args:\n",
    "        data (RDD de np.ndarray): un `RDD` de arrays NumPy.\n",
    "        k (int): El número de componentes principales a devolver.\n",
    "\n",
    "    Returns:\n",
    "        tuplas de  (np.ndarray, RDD de np.ndarray, np.ndarray): Una tupla de \n",
    "        (autovectores, `RDD` de puntuaciones, autovalores). Autovectores es un array multidimensional donde el \n",
    "        número de filas es igual a la longitud de los arrays del `RDD` de entrada y el número de columnas es\n",
    "        igual a `k`. El `RDD` de puntuaciones tiene el mismo número de filas que `data`y consiste de arrays de\n",
    "        longitud `k`. Los autovalores es un array de longitud d (el número de características).\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "    # Devuelve las `k`componentes principales, `k` puntuaciones y todos los autovalores\n",
    "    <RELLENA>\n",
    "\n",
    "# Ejecuta pca sobre correlatedData con k = 2\n",
    "topComponentsCorrelated, correlatedDataScoresAuto, eigenvaluesCorrelated = <RELLENA>\n",
    "\n",
    "# El primer elemento de los componentes principales esta en la primera columna\n",
    "print 'topComponentsCorrelated: \\n{0}'.format(topComponentsCorrelated)\n",
    "print ('\\ncorrelatedDataScoresAuto (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, correlatedDataScoresAuto.take(3)))))\n",
    "print '\\neigenvaluesCorrelated: \\n{0}'.format(eigenvaluesCorrelated)\n",
    "\n",
    "# Crear un set de test con dimensionalidad más alta\n",
    "pcaTestData = sc.parallelize([np.arange(x, x + 4) for x in np.arange(0, 20, 4)])\n",
    "componentsTest, testScores, eigenvaluesTest = pca(pcaTestData, 3)\n",
    "\n",
    "print '\\npcaTestData: \\n{0}'.format(np.array(pcaTestData.collect()))\n",
    "print '\\ncomponentsTest: \\n{0}'.format(componentsTest)\n",
    "print ('\\ntestScores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, testScores.take(3)))))\n",
    "print '\\neigenvaluesTest: \\n{0}'.format(eigenvaluesTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Función de PCA (2a)\n",
    "Test.assertTrue(checkBasis(topComponentsCorrelated.T,\n",
    "                           [[0.68915649,  0.72461254], [-0.72461254, 0.68915649]]),\n",
    "                'valor incorrecto para topComponentsCorrelated')\n",
    "firstThreeCorrelated = [[70.51682806, 69.30622356, 71.13588168], [1.48305648, 1.5888655, 1.86710679]]\n",
    "Test.assertTrue(np.allclose(firstThreeCorrelated,\n",
    "                            np.vstack(np.abs(correlatedDataScoresAuto.take(3))).T),\n",
    "                'valor incorrecto para firstThreeCorrelated')\n",
    "Test.assertTrue(np.allclose(eigenvaluesCorrelated, [1.94345403, 0.13820481]),\n",
    "                           'valores incorrectos para eigenvaluesCorrelated')\n",
    "topComponentsCorrelatedK1, correlatedDataScoresK1, eigenvaluesCorrelatedK1 = pca(correlatedData, 1)\n",
    "Test.assertTrue(checkBasis(topComponentsCorrelatedK1.T, [0.68915649,  0.72461254]),\n",
    "               'valores incorrectos para los componentes cuando k=1')\n",
    "Test.assertTrue(np.allclose([70.51682806, 69.30622356, 71.13588168],\n",
    "                            np.vstack(np.abs(correlatedDataScoresK1.take(3))).T),\n",
    "                'valor incorrecto para las puntuaciones cuando k=1')\n",
    "Test.assertTrue(np.allclose(eigenvaluesCorrelatedK1, [1.94345403, 0.13820481]),\n",
    "                           'valores incorrectos para los autovalores cuando k=1')\n",
    "Test.assertTrue(checkBasis(componentsTest.T[0], [ .5, .5, .5, .5]),\n",
    "                'valor incorrecto para componentsTest')\n",
    "Test.assertTrue(np.allclose(np.abs(testScores.first()[0]), 3.),\n",
    "                'valor incorrecto para testScores')\n",
    "Test.assertTrue(np.allclose(eigenvaluesTest, [ 128, 0, 0, 0 ]), 'valor incorrecto para eigenvaluesTest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) PCA en `dataRandom`**\n",
    "#### A continuación usa la función PCA para encontrar los dos componentes principales más altos del `dataRandom` esférico que creamos en la visualización 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "randomData = sc.parallelize(dataRandom)\n",
    "\n",
    "# Usa pca en randomData\n",
    "topComponentsRandom, randomDataScoresAuto, eigenvaluesRandom = <RELLENA>\n",
    "\n",
    "print 'topComponentsRandom: \\n{0}'.format(topComponentsRandom)\n",
    "print ('\\nrandomDataScoresAuto (tres primeros): \\n{0}'\n",
    "       .format('\\n'.join(map(str, randomDataScoresAuto.take(3)))))\n",
    "print '\\neigenvaluesRandom: \\n{0}'.format(eigenvaluesRandom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST PCA en `dataRandom` (2b)\n",
    "Test.assertTrue(checkBasis(topComponentsRandom.T,\n",
    "                           [[-0.2522559 ,  0.96766056], [-0.96766056,  -0.2522559]]),\n",
    "                'valor incorrecto para topComponentsRandom')\n",
    "firstThreeRandom = [[36.61068572,  35.97314295,  35.59836628],\n",
    "                    [61.3489929 ,  62.08813671,  60.61390415]]\n",
    "Test.assertTrue(np.allclose(firstThreeRandom, np.vstack(np.abs(randomDataScoresAuto.take(3))).T),\n",
    "                'valor incorrecto par randomDataScoresAuto')\n",
    "Test.assertTrue(np.allclose(eigenvaluesRandom, [1.4204546, 0.99521397]),\n",
    "                            'valor incorrecto para eigenvaluesRandom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 2: Proyección PCA**\n",
    "#### Traza los datos originales y la reconstrucción unidimensional usando el componente principal más alto para ver como es la solución con PCA. Los datos originales se pintan como antes y la reconstrucción unidimensional (proyección) se pinta en verde sobre los datos originales y los vectores (líneas) que representan los dos componentes principales que se muestran como líneas punteadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def projectPointsAndGetLines(data, components, xRange):\n",
    "    \"\"\" Proyecta los datos originales en la primera componente y obtiene detalles de la línea par los dos\n",
    "    componentes más altos.\"\"\"\n",
    "    topComponent= components[:, 0]\n",
    "    slope1, slope2 = components[1, :2] / components[0, :2]\n",
    "\n",
    "    means = data.mean()[:2]\n",
    "    demeaned = data.map(lambda v: v - means)\n",
    "    projected = demeaned.map(lambda v: (v.dot(topComponent) /\n",
    "                                        topComponent.dot(topComponent)) * topComponent)\n",
    "    remeaned = projected.map(lambda v: v + means)\n",
    "    x1,x2 = zip(*remeaned.collect())\n",
    "\n",
    "    lineStartP1X1, lineStartP1X2 = means - np.asarray([xRange, xRange * slope1])\n",
    "    lineEndP1X1, lineEndP1X2 = means + np.asarray([xRange, xRange * slope1])\n",
    "    lineStartP2X1, lineStartP2X2 = means - np.asarray([xRange, xRange * slope2])\n",
    "    lineEndP2X1, lineEndP2X2 = means + np.asarray([xRange, xRange * slope2])\n",
    "\n",
    "    return ((x1, x2), ([lineStartP1X1, lineEndP1X1], [lineStartP1X2, lineEndP1X2]),\n",
    "            ([lineStartP2X1, lineEndP2X1], [lineStartP2X2, lineEndP2X2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    projectPointsAndGetLines(correlatedData, topComponentsCorrelated, 5)\n",
    "\n",
    "# genera el layout y dibuja los datos\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(dataCorrelated[:,0], dataCorrelated[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((x1, x2), (line1X1, line1X2), (line2X1, line2X2)) = \\\n",
    "    projectPointsAndGetLines(randomData, topComponentsRandom, 5)\n",
    "\n",
    "# genera el layout y dibuja los datos\n",
    "fig, ax = preparePlot(np.arange(46, 55, 2), np.arange(46, 55, 2), figsize=(7, 7))\n",
    "ax.set_xlabel(r'Simulated $x_1$ values'), ax.set_ylabel(r'Simulated $x_2$ values')\n",
    "ax.set_xlim(45.5, 54.5), ax.set_ylim(45.5, 54.5)\n",
    "plt.plot(line1X1, line1X2, linewidth=3.0, c='#8cbfd0', linestyle='--')\n",
    "plt.plot(line2X1, line2X2, linewidth=3.0, c='#d6ebf2', linestyle='--')\n",
    "plt.scatter(dataRandom[:,0], dataRandom[:,1], s=14**2, c='#d6ebf2',\n",
    "            edgecolors='#8cbfd0', alpha=0.75)\n",
    "plt.scatter(x1, x2, s=14**2, c='#62c162', alpha=.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 3: Datos tridimensionales**\n",
    "#### Hasta ahora hemos trabajado con datos bidimensionales, vamos a generar ahora datos tridimensionales con características altamente correladas. Como en la visualización 1, crearemos muestras de una distribución gausiana multivariante que requiere especificar tres medias, tres varianzas y tres covarianzas.\n",
    "\n",
    "#### En los gráficos en 3D que se muestran abajo hemos incluido el plano en 2D que se corresponde con las dos componentes principales más altas (el plano con la menor distancia euclídea entre los puntos y el mismo). Los puntos de datos aunque están en un espacio tridimensional se encuentran cerca del plano 2D: El gráfico de la izquierda muestra como la mayoria de los puntos estan cerca del plano, el gráfico de la derecha muestra como el plano cubre la mayor parte de la varianza de los datos. Los puntos azul oscuro son los valores más altos en la tercera dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "m = 100\n",
    "mu = np.array([50, 50, 50])\n",
    "r1_2 = 0.9\n",
    "r1_3 = 0.7\n",
    "r2_3 = 0.1\n",
    "sigma1 = 5\n",
    "sigma2 = 20\n",
    "sigma3 = 20\n",
    "c = np.array([[sigma1 ** 2, r1_2 * sigma1 * sigma2, r1_3 * sigma1 * sigma3],\n",
    "             [r1_2 * sigma1 * sigma2, sigma2 ** 2, r2_3 * sigma2 * sigma3],\n",
    "             [r1_3 * sigma1 * sigma3, r2_3 * sigma2 * sigma3, sigma3 ** 2]])\n",
    "np.random.seed(142)\n",
    "dataThreeD = np.random.multivariate_normal(mu, c, m)\n",
    "\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "norm = Normalize()\n",
    "cmap = get_cmap(\"Blues\")\n",
    "clrs = cmap(np.array(norm(dataThreeD[:,2])))[:,0:3]\n",
    "\n",
    "fig = plt.figure(figsize=(11, 6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.azim=-100\n",
    "ax.scatter(dataThreeD[:,0], dataThreeD[:,1], dataThreeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-15, 10, 1), np.arange(-50, 30, 1))\n",
    "normal = np.array([0.96981815, -0.188338, -0.15485978])\n",
    "z = (-normal[0] * xx - normal[1] * yy) * 1. / normal[2]\n",
    "xx = xx + 50\n",
    "yy = yy + 50\n",
    "z = z + 50\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.10)\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.azim=10\n",
    "ax.elev=20\n",
    "#ax.dist=8\n",
    "ax.scatter(dataThreeD[:,0], dataThreeD[:,1], dataThreeD[:,2], c=clrs, s=14**2)\n",
    "\n",
    "ax.set_zlim((-20, 120)), ax.set_ylim((-20, 100)), ax.set_xlim((30, 75))\n",
    "ax.plot_surface(xx, yy, z, alpha=.1)\n",
    "plt.tight_layout()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) De 3D a 2D**\n",
    "#### Usaremos ahora PCA para ver si podemos recuperar el plano 2D en donde están los datos. Paraleliza los datos y utiliza la función PCA previa con  $ \\scriptsize k=2 $ componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "threeDData = sc.parallelize(dataThreeD)\n",
    "componentsThreeD, threeDScores, eigenvaluesThreeD = <RELLENA>\n",
    "\n",
    "print 'componentsThreeD: \\n{0}'.format(componentsThreeD)\n",
    "print ('\\nthreeDScores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, threeDScores.take(3)))))\n",
    "print '\\neigenvaluesThreeD: \\n{0}'.format(eigenvaluesThreeD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST 3D to 2D (2c)\n",
    "Test.assertEquals(componentsThreeD.shape, (3, 2), 'forma incorrecta de componentsThreeD')\n",
    "Test.assertTrue(np.allclose(np.sum(eigenvaluesThreeD), 969.796443367),\n",
    "                'valor incorrecto para eigenvaluesThreeD')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(componentsThreeD)), 1.77238943258),\n",
    "                'valor incorrecto para componentsThreeD')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(threeDScores.take(3))), 237.782834092),\n",
    "                'valor incorrecto para threeDScores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 4: respresentación en 2D de datos 3D**\n",
    "#### Veamos como la versión en 2D de los datos captura la mayor parte de la estructura original. Los puntos azul oscuro se corresponden con los valores más altos de los datos en la 3ª dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoresThreeD = np.asarray(threeDScores.collect())\n",
    "\n",
    "# genera la plantilla y pinta los datos\n",
    "fig, ax = preparePlot(np.arange(20, 150, 20), np.arange(-40, 110, 20))\n",
    "ax.set_xlabel(r'New $x_1$ values'), ax.set_ylabel(r'New $x_2$ values')\n",
    "ax.set_xlim(5, 150), ax.set_ylim(-45, 50)\n",
    "plt.scatter(scoresThreeD[:,0], scoresThreeD[:,1], s=14**2, c=clrs, edgecolors='#8cbfd0', alpha=0.75)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2d) Explicación de la varianza**\n",
    "#### Por último vamos a cuantificar cuanta varianza se está capturando por PCA en cada unos de los 3 datasets sintéticos que hemos analizado. Para hacerlo calcularemos la fracción de la varianza retenida en los componentes principales más altos. Recuerda que los autovalores correspondientes a cada componente principal captura la varianza a lo largo de esa dirección. Si nuestros datos iniciales son  $\\scriptsize d$-dimensionales, la varianza total en nuestros datos es igual a: $ \\scriptsize \\sum_{i=1}^d \\lambda_i $, donde $\\scriptsize \\lambda_i$ es el autovalor correspondiente al  $\\scriptsize i$-ésimo componente principal. Es más, si usamos PCA con algún  $\\scriptsize k < d$ entonces podemos calcular la varianza retenida por estos componentes principales añadiendo los $\\scriptsize k$ autovalores más altos. La fracción de la varianza retenida es igual a la suma de los  $\\scriptsize k$ autovalores más altos dividida por la suma de todos los autovalores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def varianceExplained(data, k=1):\n",
    "    \"\"\"Calcula la fracción de la varianza explicada por los `k` autovectores más altos.\n",
    "\n",
    "    Args:\n",
    "        data (RDD of np.ndarray): un RDD que contiene arrays NumPy que almacenan las características de una \n",
    "            observación.\n",
    "        k: El número de componentes principales a considerar.\n",
    "\n",
    "    Returns:\n",
    "        float: Un número entro 0 y 1 que representa el porcentaje de la varianza explicada por los `k` autovalores\n",
    "            más altos.\n",
    "    \"\"\"\n",
    "    components, scores, eigenvalues = <RELLENA>\n",
    "    <RELLENA>\n",
    "\n",
    "varianceRandom1 = varianceExplained(randomData, 1)\n",
    "varianceCorrelated1 = varianceExplained(correlatedData, 1)\n",
    "varianceRandom2 = varianceExplained(randomData, 2)\n",
    "varianceCorrelated2 = varianceExplained(correlatedData, 2)\n",
    "varianceThreeD2 = varianceExplained(threeDData, 2)\n",
    "print ('Porcentaje de la varianza explicada por el primer componente de randomData: {0:.1f}%'\n",
    "       .format(varianceRandom1 * 100))\n",
    "print ('Porcentaje de la varianza explicada por ambos componentes de randomData: {0:.1f}%'\n",
    "       .format(varianceRandom2 * 100))\n",
    "print ('\\nPorcentaje de la varianza explicada por el primer componentes de correlatedData: {0:.1f}%'.\n",
    "       format(varianceCorrelated1 * 100))\n",
    "print ('Porcentaje de la varianza explicada por ambos componentes de correlatedData: {0:.1f}%'\n",
    "       .format(varianceCorrelated2 * 100))\n",
    "print ('\\nPorcentaje de la varianza explicada por el primero de los los dos componentes de threeDData: {0:.1f}%'\n",
    "       .format(varianceThreeD2 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Explicación de la varianza (2d)\n",
    "Test.assertTrue(np.allclose(varianceRandom1, 0.588017172066), 'valor incorrecto para varianceRandom1')\n",
    "Test.assertTrue(np.allclose(varianceCorrelated1, 0.933608329586),\n",
    "                'valor incorrecto para varianceCorrelated1')\n",
    "Test.assertTrue(np.allclose(varianceRandom2, 1.0), 'valor incorrecto para varianceRandom2')\n",
    "Test.assertTrue(np.allclose(varianceCorrelated2, 1.0), 'valor incorrecto para varianceCorrelated2')\n",
    "Test.assertTrue(np.allclose(varianceThreeD2, 0.993967356912), 'valor incorrecto para varianceThreeD2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "### **Parte 3: Parseo, inspección y preprocesamiento de datos para PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### **Introducción a los datos**\n",
    "#### Un reto importante para la neurociencia es el entender como están organizadas y como funcionan las neuronas que son las células responsables del procesamiento y representación de la información en el cerebro. Las nuevas tecnología hacen posible monitorizar la respuesta de grandes cantidades de neuronas en animales vivos. Las neuronas se comunican mediante impulsos eléctricos que son registrados con electrodos lo cual no siempre es fácil de hacer, como alternativa se puede modificar geneticamente animales para que dispongan de proteinas especiales luminiscentes que se activan cuando hay actividad en las neuronas y utilizar un microscopio para registrar esa actividad neuronal. Recientemente se ha desarrollado un método llamado microscopía light-sheet que permite en un animal transparente como el pez cebra registrar todo su cerebro. Los datos resultantes son imágenes a lo largo de un tiempo de medida que contienen la actividad de miles de neuronas. El dataset en bruto es enorme y la idea es encontrar patrones de representación más compactos tanto espaciales como temporales: ¿Qué grupos de neuronas se activan juntas? ¿Cuál es la evolución temporal de su actividad? ¿Son esos patrones específicos a los eventos particulares que suceden durante el experimento ? PCA es una herramienta potente para encontrar patrones espaciales y temporales en esta clase de datos asi que ¡La vamos utilizar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) Carga de datos**\n",
    "#### En las siguientes secciones usaremos PCA para capturar la estructura de datasets neuronales. Antes de hacer el análisis cargaremos y haremos alguna inspección básica sobre los datos. Los datos en bruto están almacenados en un fichero de texto. Cada línea in el fichero contiene las series temporales de cambios de intensidad de un único pixel sobre una imagen que van variando con el tiempo (como en una película). Los dos primeros números de cada línea son las coordenadas espaciales del pixel y el resto de valores son la serie temporal. usa first() para inpeccionar una única fila e imprime rlos 100 primeros caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "baseDir = os.path.join('data')\n",
    "inputPath = os.path.join('utad-spark-ml', 'neuro.txt')\n",
    "\n",
    "inputFile = os.path.join(baseDir, inputPath)\n",
    "\n",
    "lines = sc.textFile(inputFile)\n",
    "print lines.first()[0:100]\n",
    "\n",
    "# comprobación de carga correcta\n",
    "assert len(lines.first()) == 1397\n",
    "assert lines.count() == 46460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3b) Parseo de los datos**\n",
    "#### Vamos a parsear los datos en una representación clave-valor. Queremos que cada clave sea una tupla de las coordenadas espaciales bidimensionales y cada valor sea un array NumPy que almacene la serie temporal asociada. Escribe una función que convierta una línea de texto en un par (`tuple`, `np.ndarray`) y aplica esta función a cada registro en el RDD e inspecciona la primera entrada en el nuevo dataset parseado. Ahora sería un buen momento para cachear los datos y forzar el cálculo llamando a count para asegurarnos de que los datos se han cacheado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def parse(line):\n",
    "    \"\"\" Parsea los datos en bruto en un par (`tuple`, `np.ndarray`).\n",
    "\n",
    "    Note:\n",
    "        Deberías almacenar las coordenadas del pixel en una tupla de dos interos y los elementos  de la serie\n",
    "        temporal de la intensidad del pixe. como floats en un np.ndarray.\n",
    "\n",
    "    Args:\n",
    "        line (str): Un string que representa una observación. Los elementos estan separados por espacios.\n",
    "            Los primeros dos elementos representan las coordenadas del pixel y el resto de los elementos \n",
    "            representan la intensidad del pixel a lo largo del tiempo.\n",
    "\n",
    "    Returns:\n",
    "        tuple of tuple, np.ndarray: una `tupla` (coordenada, array de intensidad del pixel) donde coordenada es\n",
    "        una `tupla` que contiene dos valores y la intensidad del pixel se almacena en un array NumPy que contiene\n",
    "        240 valores.\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "\n",
    "rawData = lines.map(parse)\n",
    "rawData.cache()\n",
    "entry = rawData.first()\n",
    "print 'La longitud de la película es de {0} segundos'.format(len(entry[1]))\n",
    "print 'El número de pixels en la pelicula es de {0:,}'.format(rawData.count())\n",
    "print ('\\nPrimera entrada de los datos en bruto (sólo los 5 primeros valores):\\n({0}, {1})'\n",
    "       .format(entry[0], entry[1][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Parseo de datos (3b)\n",
    "Test.assertTrue(isinstance(entry[0], tuple), \"La clave debería ser una tupla\")\n",
    "Test.assertEquals(len(entry), 2, 'La entrada deberia tener una entrada y un valor')\n",
    "Test.assertTrue(isinstance(entry[0][1], int), 'la tupla de coordenadas deberia contener enteros')\n",
    "Test.assertEquals(len(entry[0]), 2, \"La clave debería tener dos valores\")\n",
    "Test.assertTrue(isinstance(entry[1], np.ndarray), \"los valores debería ser un np.ndarray\")\n",
    "Test.assertTrue(isinstance(entry[1][0], np.float), 'el np.ndarray debe estar compuesto por floats')\n",
    "Test.assertEquals(entry[0], (0, 0), 'clave incorrecta ')\n",
    "Test.assertEquals(entry[1].size, 240, 'longitud de array incorrecta')\n",
    "Test.assertTrue(np.allclose(np.sum(entry[1]), 24683.5), 'valores incorrectos en el array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3c) Fluorescencia Mínima y Máxima**\n",
    "#### A continuación vamos a hacer un preprocesado básico de los datos. Los datos de la serie temporal en bruto están  en unidades de fluorenscencia y la baseline de fluerescencia varía de manera arbitraria de un pixel a otro. Primeros calcula el mínimo y el máximo  de fluorescencia de todos los pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "mn = <RELLENA>\n",
    "mx = <RELLENA>\n",
    "\n",
    "print mn, mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Min y max de fluorescencia(3c)\n",
    "Test.assertTrue(np.allclose(mn, 100.6), 'valor incorrecto para mn')\n",
    "Test.assertTrue(np.allclose(mx, 940.8), 'valor incorrecto para mx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 5: Intensidad de Pixel**\n",
    "#### Vamos a ver como un pixel aleatorio cambia de valor a lo largo de la serie temporal. Tomaremos un pixel que tenga una desviación normal de mas de 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = rawData.filter(lambda (k, v): np.std(v) > 100).values().first()\n",
    "\n",
    "# generar la regilla y dibujar los datos\n",
    "fig, ax = preparePlot(np.arange(0, 300, 50), np.arange(300, 800, 100))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'flouresence')\n",
    "ax.set_xlim(-20, 270), ax.set_ylim(270, 730)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3d) Cambio de señal fraccionada**\n",
    "#### Para convertir las unidades de fluorescencia en unidades de cambio de señal fraccionada, escribe una función que tome una serie temporal para un pixel en particular y le reste y le divida la media, después aplica esta función a todos los pixels. Confirma que el los valores máximo y mínimo han cambiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "def rescale(ts):\n",
    "    \"\"\" Toma un np.ndarray y devuelve el array normalizado restando y dividiendo por la media.n.\n",
    "\n",
    "    Note:\n",
    "        Primero resta la media y después divídelo por la media.\n",
    "\n",
    "    Args:\n",
    "        ts (np.ndarray): Serie temporal de (`np.float`) que representa la intensidad del pixel.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: La serie temporal ajustada restando la media y dividiendo por la media.\n",
    "    \"\"\"\n",
    "    <RELLENA>\n",
    "\n",
    "scaledData = rawData.mapValues(lambda v: rescale(v))\n",
    "mnScaled = scaledData.map(lambda (k, v): v).map(lambda v: min(v)).min()\n",
    "mxScaled = scaledData.map(lambda (k, v): v).map(lambda v: max(v)).max()\n",
    "print mnScaled, mxScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Cambio de señal fraccionada (3d)\n",
    "Test.assertTrue(isinstance(scaledData.first()[1], np.ndarray), 'tipo incorrecto en la rescala')\n",
    "Test.assertTrue(np.allclose(mnScaled, -0.27151288), 'valor incorrecto para mnScaled')\n",
    "Test.assertTrue(np.allclose(mxScaled, 0.90544876), 'valor incorrecto para mxScaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 6: Datos Normalizados**\n",
    "#### Ahora que hemos normalizado los datos veamos otra vez como el pixel aleatorio cambia de valor a lo largo de la serie temporal.  Vamos a visualizar un pixel que tenga una desviación estandar de más de 0.1. Nota el cambio de escala en el eje de las y comparado con la anterior visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = scaledData.filter(lambda (k, v): np.std(v) > 0.1).values().first()\n",
    "\n",
    "# Genera la parrilla y dibuja los datos\n",
    "fig, ax = preparePlot(np.arange(0, 300, 50), np.arange(-.1, .6, .1))\n",
    "ax.set_xlabel(r'time'), ax.set_ylabel(r'flouresence')\n",
    "ax.set_xlim(-20, 260), ax.set_ylim(-.12, .52)\n",
    "plt.plot(range(len(example)), example, c='#8cbfd0', linewidth='3.0')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3e) PCA en los datos escalados**\n",
    "#### Ahora que hemos preprocesado  el dataset con $\\scriptsize n = 46460$ pixels y $\\scriptsize d = 240$ segundos en una serie temporal para cada pixel podemos interpretar los pixels como nuestras observaciones y cada valor de la serie temporal como una característica. Nos gustaría encontrar patrones en la actividad cerebral durante esta serie temporal y encontrar correlaciones a lo largo del tiempo. Podemos utilizar PCA para encontrar una representación más compacta de nuestros datos que nos permitan visualizarlos.\n",
    "\n",
    "#### Usa la función `pca` del apartado (2a) para realizar PCA  en el dataset de neurociencia con  $\\scriptsize k = 3$. La función `pca` toma un RDD de arrays pero `data` es un RDD de pares clave-valor asi que necesitarás extraer los valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# Ejecuta pca usando scaledData\n",
    "componentsScaled, scaledScores, eigenvaluesScaled = <RELLENA>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST PCA en los datos escalados (3e)\n",
    "Test.assertEquals(componentsScaled.shape, (240, 3), 'forma incorrecta para componentsScaled')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(componentsScaled[:5, :])), 0.283150995232),\n",
    "                'valor incorrecto para componentsScaled')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(scaledScores.take(3))), 0.0285507449251),\n",
    "                'valor incorrecto para scaledScores')\n",
    "Test.assertTrue(np.allclose(np.sum(eigenvaluesScaled[:5]), 0.206987501564),\n",
    "                'valor incorrecto para eigenvaluesScaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 7: Los dos mejores componentes como imágenes**\n",
    "#### Ahora veremos las puntuaciones para los dos componentes más altos como imágenes. Transformamos los vectores de sus dimensiones originales a un tamaño de imagen de 230 x 202\n",
    "\n",
    "#### Estos gráficos mapean los valores para un único componente a una imagen en escala de grises lo que nos proporciona una representación visual para poder ver la estructura general del cerebro del pez cebra e identificar donde ocurren los valores más altos y más bajos. No obstante hay una cantidad substancial de información útil que es dificil de interpretar. En la siguiente visualización veremos como podemos mejorar la interpretación combinando las dos componentes principales más altas en una única imagen utilizando un mapa de colores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "scoresScaled = np.vstack(scaledScores.collect())\n",
    "imageOneScaled = scoresScaled[:,0].reshape(230, 202).T\n",
    "\n",
    "# generar la parrillar y dibujar los datos\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hideLabels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Top Principal Component', color='#888888')\n",
    "image = plt.imshow(imageOneScaled,interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imageTwoScaled = scoresScaled[:,1].reshape(230, 202).T\n",
    "\n",
    "# generar la parrillar y dibujar los datos\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hideLabels=True)\n",
    "ax.grid(False)\n",
    "ax.set_title('Second Principal Component', color='#888888')\n",
    "image = plt.imshow(imageTwoScaled,interpolation='nearest', aspect='auto', cmap=cm.gray)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualización 8: Los dos mejores componentes como una única imagen**\n",
    "#### Cuando realizamos PCA y damos color a las neuronas basadas en su localización en un espacio de pocas dimesiones (2/3 dimensiones) podemos interpretar áreas con colores similares como zonas que muestran respuestas similares (al menos en términos de una representación simple que podamos recuperar con PCA). Abajo se muestra en el primer gráfico las dos componentes principales mapeadas a colores. Y en el segundo y tercer gráficos se muestran el resultado de este mapeo de color sobre todos los datos neuronales del pez cebra.\n",
    " \n",
    "#### El segundo y tercer gráficos también se muestran claramente los patrones de similitud neuronal a lo largo de diferentes regiones del cerebro. Sin embargo cuando se realiza PCA sobre todo el dataset hay múltiples razones por las que las neuronas pueden tener respuestas similares. Las neuronas pueden responder de manera similar a diferentes estímulos, sus respuestas pueden tener dinámicas temporales similares o su similitud en la respuesta podría estar influenciada tanto por factores específicos de los estímulos como temporales. No obstante con nuestro análisis inicial PCA no podemos precisar los factores subyacentes y por lo tanto es dificil interpretar que significa realmente \"similitud\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detalles opcionales: Usamos [coordenadas polares](https://en.wikipedia.org/wiki/Polar_coordinate_system) para mapear los puntos de colores. Para usar coordenadas polares es necesario proporcionar un ángulo $ (\\phi) $ y una magnitud $ (\\rho) $. Después vamos a usar el espacio de color polar HSV [hue-saturation-value](https://en.wikipedia.org/wiki/HSL_and_HSV) para mapear el ángulo a una tonalidad (hue)  y la magnitud a un valor (brillo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from python-thunder's Colorize.transform where cmap='polar'.\n",
    "# Checkout the library at: https://github.com/thunder-project/thunder and\n",
    "# http://thunder-project.org/\n",
    "\n",
    "def polarTransform(scale, img):\n",
    "    \"\"\"Convert points from cartesian to polar coordinates and map to colors.\"\"\"\n",
    "    from matplotlib.colors import hsv_to_rgb\n",
    "\n",
    "    img = np.asarray(img)\n",
    "    dims = img.shape\n",
    "\n",
    "    phi = ((np.arctan2(-img[0], -img[1]) + np.pi/2) % (np.pi*2)) / (2 * np.pi)\n",
    "    rho = np.sqrt(img[0]**2 + img[1]**2)\n",
    "    saturation = np.ones((dims[1], dims[2]))\n",
    "\n",
    "    out = hsv_to_rgb(np.dstack((phi, saturation, scale * rho)))\n",
    "\n",
    "    return np.clip(out * scale, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Muestra la transformación polar de los componentes principales a colores.\n",
    "x1AbsMax = np.max(np.abs(imageOneScaled))\n",
    "x2AbsMax = np.max(np.abs(imageTwoScaled))\n",
    "\n",
    "numOfPixels = 300\n",
    "x1Vals = np.arange(-x1AbsMax, x1AbsMax, (2 * x1AbsMax) / numOfPixels)\n",
    "x2Vals = np.arange(x2AbsMax, -x2AbsMax, -(2 * x2AbsMax) / numOfPixels)\n",
    "x2Vals.shape = (numOfPixels, 1)\n",
    "\n",
    "x1Data = np.tile(x1Vals, (numOfPixels, 1))\n",
    "x2Data = np.tile(x2Vals, (1, numOfPixels))\n",
    "\n",
    "# Try changing the first parameter to lower values\n",
    "polarMap = polarTransform(2.0, [x1Data, x2Data])\n",
    "\n",
    "gridRange = np.arange(0, numOfPixels + 25, 25)\n",
    "fig, ax = preparePlot(gridRange, gridRange, figsize=(9.0, 7.2), hideLabels=True)\n",
    "image = plt.imshow(polarMap, interpolation='nearest', aspect='auto')\n",
    "ax.set_xlabel('Principal component one'), ax.set_ylabel('Principal component two')\n",
    "gridMarks = (2 * gridRange / float(numOfPixels) - 1.0)\n",
    "x1Marks = x1AbsMax * gridMarks\n",
    "x2Marks = -x2AbsMax * gridMarks\n",
    "ax.get_xaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x1Marks))\n",
    "ax.get_yaxis().set_ticklabels(map(lambda x: '{0:.1f}'.format(x), x2Marks))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Misma transformación sobre los datos de la imagen con el primer parámetros establecido a valores más bajos\n",
    "brainmap = polarTransform(2.0, [imageOneScaled, imageTwoScaled])\n",
    "\n",
    "# generación de parrilla y dibujo de datos\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hideLabels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Parte 4: Agregación basada en características seguida de PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4a) Agregación usando arrays**\n",
    " \n",
    "#### En el análisis de la parte 3 realizamos PCA sobre toda la serie temporal intentando encontrar patrones globales durante los 240 segundos. Sin embargo nuestro análisis no tiene en cuenta el hecho de que han sucedido diferentes eventos durante esos 240 segundos. En concreto  se le han presentado al pez cebra 12 patrones visuales y cada uno de ellos ha durado 20 segundos haciendo un total de 12 x 20 = 240 características. Seguramente tendremos tendremos mejores patrones si incorporamos el conocimiento de los experimentos dentro de los análisis. Como veremos podemos aislar el impacto de la respuesta temporal agregando de manera apropiada las características.\n",
    " \n",
    "#### Para agregar las características vamos a usar multiplicación de matrices, si usamos  `np.dot` en un array bidimensional NumPy el resultado sería equivalente a una multiplicación de matrices. Por ejemplo `np.array([[1, 2, 3], [4, 5, 6]]).dot(np.array([2, 0, 1]))` devuelve `np.array([5, 14])`.\n",
    "#### $$\\begin{bmatrix} 1 & 2 & 3 \\\\\\ 4 & 5 & 6 \\end{bmatrix} \\begin{bmatrix} 2 \\\\\\ 0 \\\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\\\ 14 \\end{bmatrix} $$\n",
    "#### Si se crea correctamente el array multidimensional podemos multiplicarlo por un vector para realizar operaciones de agregación. Imagina, por ejemplo, que tenemos un vector de 3 dimensiones $ \\scriptsize \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}^\\top $  y queremos crear un vector de dos dimensiones que contenga la suma de su primer y último elementos como un único valor y tres veces su segundo valor como otro valor i.e. $ \\scriptsize \\begin{bmatrix} 4 & 6 \\end{bmatrix}^\\top $. Podemos obtener este resultado haciendo una multiplicación de matrices:  `np.array([[1, 0, 1], [0, 3, 0]]).dot(np.array([1, 2, 3])` lo que devuelve `np.array([4, 6]`.\n",
    "#### $$\\begin{bmatrix} 1 & 0 & 1 \\\\\\ 0 & 3 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\\\ 2 \\\\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\\\ 6 \\end{bmatrix} $$\n",
    "#### En este ejercicio crearás varios arrays que realicen diferentes tipos de agregación. La agregación se especifica en los comentarios de cada array. Rellena primero los valores del array a mano y automatizaremos la creación del array en los siguientes dos ejercicios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "vector = np.array([0., 1., 2., 3., 4., 5.])\n",
    "\n",
    "#Crea un array multidimensional que al multiplicarlo por un vector (usando .dot) devuelva un\n",
    "#array de dos elementos donde el primer elemento sea la suma de los elementos de los índices 0, 2 y 4\n",
    "#del vector y el segundo elemento sea la suma de los elmentos de los índices 1, 3, y 5.\n",
    "#Debería ser un array de 2 filas y 6 columnas\n",
    "sumEveryOther = np.array(<RELLENA>)\n",
    "\n",
    "#crea un array multidimensional que al multiplicarlo por un vector (usando .dot) devuelva un array de 3 elementos\n",
    "#donde el primer elemento sea la suma de los elementos de los índices 0 y 3, el segundo elemento sea la suma\n",
    "#de los elementos de los índices 1 y 4 y el tercer elemento sea la suma de los elementos de los índices 2 y 5\n",
    "#debería de ser un array de 3 filas y 6 columnas\n",
    "sumEveryThird = np.array(<RELLENA>)\n",
    "\n",
    "#Crea un array multidimensional que sirva para sumar los primeros 3 elementos y los\n",
    "#y 3 últimos elementos del vector y que devuelva un array de dos elementos utilizando .dot\n",
    "#debería ser un array de 2 filas y 6 columnas\n",
    "\n",
    "sumByThree = np.array(<RELLENA>)\n",
    "\n",
    "#Crea un array multidimensional que sume los primeros dos elementos, los segundos dos elementos y\n",
    "#los últimos dos elementos y que devuelve un array de 3 elementos utilizando .dot\n",
    "#debería ser un array de 3 filas y 6 columnas\n",
    "\n",
    "sumByTwo = np.array(<RELLENA>)\n",
    "\n",
    "print 'sumEveryOther.dot(vector):\\t{0}'.format(sumEveryOther.dot(vector))\n",
    "print 'sumEveryThird.dot(vector):\\t{0}'.format(sumEveryThird.dot(vector))\n",
    "\n",
    "print '\\nsumByThree.dot(vector):\\t{0}'.format(sumByThree.dot(vector))\n",
    "print 'sumByTwo.dot(vector): \\t{0}'.format(sumByTwo.dot(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Agregación usando arrays (4a)\n",
    "Test.assertEquals(sumEveryOther.shape, (2, 6), 'forma incorrecta para sumEveryOther')\n",
    "Test.assertEquals(sumEveryThird.shape, (3, 6), 'forma incorrecta para sumEveryThird')\n",
    "Test.assertTrue(np.allclose(sumEveryOther.dot(vector), [6, 9]), 'valor incorrecto para sumEveryOther')\n",
    "Test.assertTrue(np.allclose(sumEveryThird.dot(vector), [3, 5, 7]),\n",
    "                'valor incorrecto para sumEveryThird')\n",
    "Test.assertEquals(sumByThree.shape, (2, 6), 'valor incorrecto para sumByThree')\n",
    "Test.assertEquals(sumByTwo.shape, (3, 6), 'valor incorrecto para sumByTwo')\n",
    "Test.assertTrue(np.allclose(sumByThree.dot(vector),  [3, 12]), 'valor incorrecto para sumByThree')\n",
    "Test.assertTrue(np.allclose(sumByTwo.dot(vector), [1, 5, 9]), 'valor incorrecto para sumByTwo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4b) Uso de `np.tile` y `np.eye`**\n",
    "#### [np.tile](http://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html) es útil para repetir array en una o más dimensiones, por ejemplo, `np.tile(np.array([[1, 2], [3, 4]]), 2)` devuelve `np.array([[1, 2, 1, 2], [3, 4, 3, 4]]))`.\n",
    "####  $$ np.tile( \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} , 2) \\to \\begin{bmatrix} 1 & 2 & 1& 2 \\\\\\ 3 & 4 & 3 & 4 \\end{bmatrix} $$\n",
    "#### [np.eye](http://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html) se puede usar para crear el array identidad $ (\\mathbf{I_n}) $.  Por ejemplo, `np.eye(3)` devuelve `np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])`.\n",
    "#### $$ np.eye( 3 ) \\to \\begin{bmatrix} 1 & 0 & 0 \\\\\\ 0 & 1 & 0 \\\\\\ 0 & 0 & 1 \\end{bmatrix} $$\n",
    "#### En este ejercicio vamos a implementar  `sumEveryOther` y `sumEveryThird` usando `np.tile` y `np.eye`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# referencia de lo que vamos a reimplementar\n",
    "print 'sumEveryOther: \\n{0}'.format(sumEveryOther)\n",
    "print '\\nsumEveryThird: \\n{0}'.format(sumEveryThird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# usa np.tile y np.eye para reconstruir los arrays\n",
    "sumEveryOtherTile = <RELLENA>\n",
    "sumEveryThirdTile = <RELLENA>\n",
    "\n",
    "print sumEveryOtherTile\n",
    "print 'sumEveryOtherTile.dot(vector): {0}'.format(sumEveryOtherTile.dot(vector))\n",
    "print '\\n', sumEveryThirdTile\n",
    "print 'sumEveryThirdTile.dot(vector): {0}'.format(sumEveryThirdTile.dot(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST reconstrucción con `np.tile` y `np.eye` (4b)\n",
    "Test.assertEquals(sumEveryOtherTile.shape, (2, 6), 'forma incorrecta sumEveryOtherTile')\n",
    "Test.assertEquals(sumEveryThirdTile.shape, (3, 6), 'forma incorrecta sumEveryThirdTile')\n",
    "Test.assertTrue(np.allclose(sumEveryOtherTile.dot(vector), [6, 9]),\n",
    "                'valor incorrecto sumEveryOtherTile')\n",
    "Test.assertTrue(np.allclose(sumEveryThirdTile.dot(vector), [3, 5, 7]),\n",
    "                'valor incorrecto sumEveryThirdTile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4c) Reconstrucción con `np.kron` **\n",
    "#### El producto Kronecker es la generalización del outer product sobre matrices, abajo tienes ejemplos para ilustrar la idea. En [Wikipedia](https://en.wikipedia.org/wiki/Kronecker_product) tienes una definición más detallada.  Podemos usar [np.kron](http://docs.scipy.org/doc/numpy/reference/generated/numpy.kron.html) para calcular los productos Kronecker y reconstruir los arrays `sumBy`.  $ \\otimes $ indica un productor Kronecker.\n",
    "\n",
    "#### $$ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 2 \\end{bmatrix}  = \\begin{bmatrix} 1 \\cdot 1 & 1 \\cdot 2 & 2 \\cdot 1 & 2 \\cdot 2 \\\\\\ 3 \\cdot 1 & 3 \\cdot 2 & 4 \\cdot 1 & 4 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 2 & 4 \\\\\\ 3 & 6 & 4 & 8 \\end{bmatrix}  $$\n",
    "####  Podemos ver como el producto Kronecker continúa expandiendose si añadimos otra fila al segundo array.\n",
    "#### $$ \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} \\otimes \\begin{bmatrix} 1 & 2 \\\\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1 & 1 \\cdot 2 & 2 \\cdot 1 & 2 \\cdot 2 \\\\\\ 1 \\cdot 3 & 1 \\cdot 4 & 2 \\cdot 3 & 2 \\cdot 4 \\\\\\ 3 \\cdot 1 & 3 \\cdot 2 & 4 \\cdot 1 & 4 \\cdot 2 \\\\\\ 3 \\cdot 3 & 3 \\cdot 4 & 4 \\cdot 3 & 4 \\cdot 4 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 2 & 4 \\\\\\ 3 & 4 & 6 & 8 \\\\\\ 3 & 6 & 4 & 8 \\\\\\ 9 & 12 & 12 & 16 \\end{bmatrix} $$\n",
    "#### En este ejercicio reconstruirás los arrays `sumByThree` y `sumByTwo` usando `np.kron`, `np.eye`, y `np.ones`.   `np.ones` crea un array de unos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# como referencia\n",
    "print 'sumByThree: \\n{0}'.format(sumByThree)\n",
    "print '\\nsumByTwo: \\n{0}'.format(sumByTwo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# usa np.kron, np.eye, y np.ones para reconstruir los arrais\n",
    "sumByThreeKron = <RELLENA>\n",
    "sumByTwoKron = <RELLENA>\n",
    "\n",
    "print sumByThreeKron\n",
    "print 'sumByThreeKron.dot(vector): {0}'.format(sumByThreeKron.dot(vector))\n",
    "print '\\n', sumByTwoKron\n",
    "print 'sumByTwoKron.dot(vector): {0}'.format(sumByTwoKron.dot(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Reconstrucción con `np.kron` (4c)\n",
    "Test.assertEquals(sumByThreeKron.shape, (2, 6), 'forma incorrecta sumByThreeKron')\n",
    "Test.assertEquals(sumByTwoKron.shape, (3, 6), 'forma incorrecta sumByTwoKron')\n",
    "Test.assertTrue(np.allclose(sumByThreeKron.dot(vector),  [3, 12]),\n",
    "                'valor incorrecto sumByThreeKron')\n",
    "Test.assertTrue(np.allclose(sumByTwoKron.dot(vector), [1, 5, 9]),\n",
    "                'valor incorrecto sumByTwoKron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4d) Agregación por tiempo** como vimos en la parte (4a) sería interesante incorporar el conocimiento experimental en nuestro análisis. Para hacerlo estudiaremos primero los aspectos temporales de la respuesta neuronal agregando nuestras características a lo largo del tiempo. En otras palabras, queremos ver como diferentes píxeles (y las neuronas capturadas en esos píxels) reacionan durante los 20 segundos en los que se muestran nuevos patrones sin importar que tipo de patrón es. Por lo que en vez de trabajar con 240 características individuales vamos a agregarlas a 20 nuevas características donde la primera característica nueva captura la respuesta un segundo despues de que aparezca un patrón visual, la segunda característica nueva es la respuesta después de dos segundos y asi sucesivamente.\n",
    "#### Podemos realizar esta agregación utilizando una operación de map. Primero construimos un array multidimensional $ \\scriptsize \\mathbf{T} $ que cuando se le aplique la operación punto con un vector de 240 dimensiones sume las componentes de cada segundo para cada una de las 12 exposiciones y devuelva un vector de 20 dimensiones. Este ejercicio es similar al (4b). Una vez creado el array multidimensional $ \\scriptsize \\mathbf{T} $, usa una operación `map` con el array y cada serie temporal para generar un dataset transformado. Después lo cachearemos y haremos una cuenta de la salida ya que vamos a utilizarlo más veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# crea un array multidimensiona para realizar la agregación\n",
    "T = <RELLENA>\n",
    "\n",
    "# Transforma scaledData usando T recuerda guardar las claves\n",
    "timeData = scaledData.<RELLENA>\n",
    "\n",
    "timeData.cache()\n",
    "print timeData.count()\n",
    "print timeData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Agregación pro tiempo (4d)\n",
    "Test.assertEquals(T.shape, (20, 240), 'forma incorrecta para T')\n",
    "timeDataFirst = timeData.values().first()\n",
    "timeDataFifth = timeData.values().take(5)[4]\n",
    "Test.assertEquals(timeData.count(), 46460, 'valor incorrecto timeData')\n",
    "Test.assertEquals(timeDataFirst.size, 20, 'valor incorrecto timeData')\n",
    "Test.assertEquals(timeData.keys().first(), (0, 0), 'valor incorrecto timeData')\n",
    "Test.assertTrue(np.allclose(timeDataFirst[:2], [0.00802155, 0.00607693]),\n",
    "                'valor incorrecto timeData')\n",
    "Test.assertTrue(np.allclose(timeDataFifth[-2:],[-0.00636676, -0.0179427]),\n",
    "                'valor incorrecto timeData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4e) Obtener una representación compacta**\n",
    "#### Ahora tenemos un dataset agregado por tiempo con  $\\scriptsize n = 46460$ pixels y $\\scriptsize d = 20$ características temporales agregadas y queremos usar PCA para encontrar una representación más compacta. Usa la función  `pca` del apartado (2a) para realicar PCA en estos datos con $\\scriptsize k = 3$ para obterner como resultado un dataset con una dimensionalidad de 40.460 x 3. Como antes necesitarás extraer los valores de `timeData` ya que es un RDD de pares clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "componentsTime, timeScores, eigenvaluesTime = <RELLENA>\n",
    "\n",
    "print 'componentsTime: (first five) \\n{0}'.format(componentsTime[:5,:])\n",
    "print ('\\ntimeScores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, timeScores.take(3)))))\n",
    "print '\\neigenvaluesTime: (first five) \\n{0}'.format(eigenvaluesTime[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST representación compacta (4e)\n",
    "Test.assertEquals(componentsTime.shape, (20, 3), 'forma incorrecta componentsTime')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(componentsTime[:5, :])), 2.37299020),\n",
    "                'valor incorrecto componentsTime')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(timeScores.take(3))), 0.0213119114),\n",
    "                'valor incorrecto timeScores')\n",
    "Test.assertTrue(np.allclose(np.sum(eigenvaluesTime[:5]), 0.844764792),\n",
    "                'valor incorrecto eigenvaluesTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Visualización 9: Los dos mejores componentes en el tiempo **\n",
    "#### Veamos la puntuación de los dos PC más altos como una imagen compuesta. Cuando preprocesamos agregando por tiempo y después realizamos PCA sólo estamos mirando la variabilidad relacionada con las dinámicas temporales, como resultado si las neuronas parecen similares (tienen colores similares) en la imagen resultante significa que sus respuestas pueden variar de manera similiar a lo largo del tiempo independientemente de la dirección de codificación. En la imagen de abajo podemos definir la línea media como la línea horizontal a lo largo del cerebro y podemos ver claramente patrones de actividad neuronal en diferentes partes del cerebro y en concreto podemos ver que las regiones a cada lado de la línea media son similares, lo que sugiere que las dinámicas temporales no difieren a lo largo de las dos partes del cerebro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoresTime = np.vstack(timeScores.collect())\n",
    "imageOneTime = scoresTime[:,0].reshape(230, 202).T\n",
    "imageTwoTime = scoresTime[:,1].reshape(230, 202).T\n",
    "brainmap = polarTransform(3, [imageOneTime, imageTwoTime])\n",
    "\n",
    "# generar la parrilla y dibujar los datos\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hideLabels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap,interpolation='nearest', aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4f) Aggregar por dirección **\n",
    "#### A continuación vamos a realizar un segundo tipo de agregación de características de modo que podamos estudiar aspectos específicos a la dirección de la respuesta neuronal. En otras palabra queremos ver como píxels diferentes (y las neuronas capturadas por esos pixels) reaccionan cuando a un pez cebra se le presentan 12 patrones específicos de dirección ignarando el aspecto temporal de la reacción. En vez de trabajar con 240 características individuales agregaremos las características originales en 12 nuevas características donde la primera característica nueva captura la respuesta del pixel medio al primer patrón específico de dirección, la segunda característica nueva es la respuesta al segundo patrón visual específico de dirección y así sucesivamente.\n",
    " \n",
    "#### Como en la parte (4c) diseñaremos un array multidimensional $ \\scriptsize \\mathbf{D} $ que cuando sea multiplicado por un vector de 240 dimensioes sume los primeros 20 componentes después los segundos 20 componentes y así sucesivamente. Este ejercicio es muy similar a (4c). Crea primero $ \\scriptsize \\mathbf{D} $ después usa una operación `map` con ese array y cada serie temporal para generar un dataset transformado. Cachearemos y contaremos la salida ya que la usaremos más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "# Crea un array multidimensional para realizar la agregación\n",
    "D = <RELLENA>\n",
    "\n",
    "# Transforma scaledData usando D, Transform scaledData using D. Recuerda mantener las claves.\n",
    "directionData = scaledData.<RELLENA>\n",
    "\n",
    "directionData.cache()\n",
    "print directionData.count()\n",
    "print directionData.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST agregar por dirección (4f)\n",
    "Test.assertEquals(D.shape, (12, 240), 'forma incorrecta para D')\n",
    "directionDataFirst = directionData.values().first()\n",
    "directionDataFifth = directionData.values().take(5)[4]\n",
    "Test.assertEquals(directionData.count(), 46460, 'longitud incorrecta de directionData')\n",
    "Test.assertEquals(directionDataFirst.size, 12, 'valor de longitud incorrecta directionData')\n",
    "Test.assertEquals(directionData.keys().first(), (0, 0), 'claves incorrectas en directionData')\n",
    "Test.assertTrue(np.allclose(directionDataFirst[:2], [ 0.03346365,  0.03638058]),\n",
    "                'valores incorrectos en directionData')\n",
    "Test.assertTrue(np.allclose(directionDataFifth[:2], [ 0.01479147, -0.02090099]),\n",
    "                'valores incorrectos en directionData')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4g) Representación compacta de los datos de dirección**\n",
    "#### Ahora tenemos un dataset agregado por dirección con $\\scriptsize n = 46460$ pixels y $\\scriptsize d = 12$ características de dirección agregadas y queremos usar PCA para encontrar un modo de representación más compacto. Usa la función de `pca` del apartado (2a) para realizar PCA en estos datos con  $\\scriptsize k = 3$, dando como resultado un dataset de 46.460 x 3. Como antes necesitarás extrer los valores de `directionData` ya que es un RDD de pares clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Sustituye <RELLENA> con el código apropiado\n",
    "componentsDirection, directionScores, eigenvaluesDirection = <RELLENA>\n",
    "\n",
    "print 'componentsDirection: (first five) \\n{0}'.format(componentsDirection[:5,:])\n",
    "print ('\\ndirectionScores (first three): \\n{0}'\n",
    "       .format('\\n'.join(map(str, directionScores.take(3)))))\n",
    "print '\\neigenvaluesDirection: (first five) \\n{0}'.format(eigenvaluesDirection[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST representación compacta de datos de dirección (4g)\n",
    "Test.assertEquals(componentsDirection.shape, (12, 3), 'forma incorrecta componentsDirection')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(componentsDirection[:5, :])), 1.080232069),\n",
    "                'valor incorrecto componentsDirection')\n",
    "Test.assertTrue(np.allclose(np.abs(np.sum(directionScores.take(3))), 0.10993162084),\n",
    "                'valor incorrecto directionScores')\n",
    "Test.assertTrue(np.allclose(np.sum(eigenvaluesDirection[:5]), 2.0089720377),\n",
    "                'valor incorrecto eigenvaluesDirection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### **Visualización 10: Los dos mejores componentes por dirección**\n",
    "#### Vamos a ver las puntuaciones de los dos PC más altos como una imagen compuesta. Cuando preprocesamos promediando a lo largo del tiempo (agrupando por dirección) y realizamos PCA solo estamos mirando la variabilidad relacionada con la dirección de los estímulos, como resultado si las neuronas parecen similares (tienen colores similares) en la imagen significa que sus respuestas varían de manera similiar a lo largo de las direcciones sin importar como evolucionan sobre el tiempo. En la imagen de abajo vemos un patrón diferente de similitud a lo largo de las regiones del cerebro. Es  más, las regiones a ambos lados de la línea media tienen colores diferentes lo que sugiere que estamos mirando a una propiedad (selección de la dirección) que tiene una representación  diferente en ambos lados del cerebro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoresDirection = np.vstack(directionScores.collect())\n",
    "imageOneDirection = scoresDirection[:,0].reshape(230, 202).T\n",
    "imageTwoDirection = scoresDirection[:,1].reshape(230, 202).T\n",
    "brainmap = polarTransform(2, [imageOneDirection, imageTwoDirection])\n",
    "# with thunder: Colorize(cmap='polar', scale=2).transform([imageOneDirection, imageTwoDirection])\n",
    "\n",
    "# generar parrila y trazar datos\n",
    "fig, ax = preparePlot(np.arange(0, 10, 1), np.arange(0, 10, 1), figsize=(9.0, 7.2), hideLabels=True)\n",
    "ax.grid(False)\n",
    "image = plt.imshow(brainmap, interpolation='nearest', aspect='auto')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4h) pasos siguientes**\n",
    "#### En el análisis anterior hemos identificado con éxito regiones del cerebro que codifican  propiedades particulares como por ejemplo un patrón temporal particular o la reacción a un estímulo. Sin embargo esto sólo es el primer paso. A estos análisis exploratorios les siguen una investigación más profunda, que podría por ejemplo implicar el uso de un problema de regresión lineal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
